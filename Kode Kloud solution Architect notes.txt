Kode Kloud solution Architect notes:

Services: Networking

VPC: (Virtual Private Cloud)
It is a secured,isolated network segment hosted within AWS.
They allow us to isolate resources from other resources in the cloud. (If we dont want applications to talk to each other)
With VPCs, we get access to subneting (IP Address),Routing (Route tables),Firewalls (NACLS and Security Groups) & Gateways.
VPCs are specific to a single region. (eg: vpc1 in us-east-1 region and vpc2 in us-east-2 region)
VPC acts as network boundary. And resources created in these vpc are completly isolated. It is also a kind of security. And if we want connection between them, we need to explicitly allow it.

Every VPC has a range of IP addresses assigned to it called CIDR block.
A CIDR block defines the IP addresses that resources in the VPC can use.
(Eg. If we have the server deployed in VPC-1, it can only get IP address within that CIDR block)

A CIDR block size can be anywhere from a /16 to a /28 .
Assume we create a VPC-1 and it has a CIDR block of 192.168.0.0/16. This means the IP addresses from 192.168.0.0 to 192.168.255.255 are avilable for our resources deployed in VPC-1.
We can also enable optional secondary IPv4 Block. and Optional IPv6 /56 CIDR Block. And can have upto 5 IPv6 CIDR blocks, but this limit is adjustable.

There are 2 types of VPCs:
1.Default VPC 
2.Custom VPC

Default VPC is automatically created by AWS in all the region by default. the services deployed in it can automatically access internet.
They always have /16 IPv4 CIDR block 172.31.0.0/16 (65,536 addresses). This is common for all regions and all accounts. We get 1 default subnet (/20) in each Availability Zone (4096 addresses). (Eg. 172.31.16.0/20 in AZ1 and 172.31.32.0/20 in AZ2).
By defaultly Internet gateway attached to the VPC. A route that points all traffic (0.0.0.0/0) to the internet gateway. Devices in these default subnets will be accessible from the internet. Default security group (allowing outbound traffic by default) and Default Nacl (allows both inbound and outbound traffic).
The Default VPC and its subnets have outbound access to the internet by default.

In custom VPC, We need to do all the settings in it.
-----------------------------------
Demo: Custom VPC

Goto AWS,select region and go to VPC. We can see default VPC here. Create vpc button
Choose option VPC only (or vpc and more).
demo-vpc --> Ipv4 CIDR block : 10.0.0.0/16 --> no IPv6 CIDR block --> add tags --> create vpc . We can see all the details of our VPC now. We can also see "Resource map" tab for it gives complete information about our VPC.
Select the VPC and click on delete.
------------------------------------
Demo: Default VPC

Go to VPC and click on the default VPC to see more information of it.
Go to subnets tab and we can see 6 default subnets in us-east-1 (N.Virginia) region. This is because we have 6 AZ in this region. Now we can go to resource map tab and visualize it. we can also see 1 default route table and 1 igw (provides internet connection to the resources attached to it) 
Since we have igw associated with this VPC, the "Auto-assign public IPv4 address" option is set to 'Yes'. This means when we deploy a server in this VPC, its going to automatically get public address and we will be automatically able to connect to it.
You can test this by launching EC2 instance in default subnet.
---------------------------------
Subnets:

Subnets are groups of IP addresses in your VPC.
A subnets resides within a single Availability Zone.
Subnets can be made public or private to allow external access to resources within them. 
Subnets within a VPC must be within the CIDR range. (if our VPC CIDR is 192.168.0.0/16, then subnet of 192.168.10.0/24 is valid but 10.100.1.0/24 is invalid as it is not within the VPC CIDR range.)
A subnet block size must be between a /16 and a /28 .
The first 4 IP addresses of a subnet are reserved and cannot be used.
i.e 192.168.10.0 (Network address), 192.168.10.1 (VPC Router), 192.168.10.2 (DNS), 192.168.10.3 (for Future Use), and last IP address for subnt (192.168.10.255) is reserved for broadcast address.

Subnet configuration options:
* Subnets cannot overlap with other subnets in the VPC 
Eg:
Subnet A: 10.16.0.0/24 and Subnet B: 10.16.0.128/25 in the same vpc is invalid
Overlapping IP Address Ranges,
Subnet A: 10.16.0.0 to 10.16.0.255
Subnet B: 10.16.0.128 to 10.16.0.255

* A subnet allows for an optional IPv6 CIDR.
* A subnet can be configured to be IPv6 only - No IPv4 addresses
* Subnets can communicate with other subnets in the VPC
* Auto-assign public IPv4/IPv6 address in addition to the private address.
--------------------------------------
Demo: Subnets

Create a new custom VPC with CIDR 10.0.0.0/16 with Amazon provided ipv6 CIDR block.
Go to subnets --> select VPC --> subnet-1 --> AZ: us-east-1d --> IPV4 CIDR block: 10.0.1.0/24  --> IPc6 CIDR block : 2600:1f18:24e0:de00::/56 --> create subnets.
Similarly create one more subnet 10.0.5.0/24 in us-east-1a region. 
Now if we want to deploy our server in subnet-1, then the server will be deployed in us-east-1d. 
Now we can test this by creating an instance in this subnet. Now we can see that the instance private ip is 10.0.5.113 which is within the subnet we specified.
Now delete the instance and delete the VPC (This will automatically delete the subnets in it).
-------------------------------------
Routing in VPC:

Every VPC has a VPC router. And this router can be accessible from a network and 1 address of each subnet. Purpose of router is to route traffic between subnets and also in and out of vpc. 
A route table is set of rules that the router uses to forward network traffic.   
Each rule in the route table is referred to as 'route'.
All route tables have one route by default i.e local route.
The router has an interface in every subnet of VPC and is reachable from the network + 1 address of each subnet. 
Every subnet is associated with route table. We can specify the route table with which the subnet is associated with. 
Suppose we have subnet in AZ1, then any traffic leaving this subnet will follow the rules in the route table and decide where the package should be sent.
When we create vpc, it will automatically create default route table for us. and the new subnets created will automatically going to get associated with this route table untill we point it to different route table.
Multiple subnets can be asociated with single route table. But a subnet can be associated with only one route table at a time. (IMP)
---------------------------------------
Demo: Route table

Create a vpc and 2 subnets. Now we can see our subnets are automatically assigned to a main route table.click on the route table.
Now we can see in the subnet association, there is no explicit subnet association and there we can see only subnet without explicit association.
Create our own route table. create routr table --> name--> choose vpc --> create
Now click on edit subnet association and choose the subnet-1 .
Now this means that any traffic that comes from subnet1 will follow the rules associated with route table 1 (rt1). We can se it in the routes tab. Similarly create one more rt2 and associate subnet 2 with it.
Now click on routes tab and edit route ,add route 0.0.0.0/0 and choose whatever target u want i.e nat,local,igw etc and save
Finally delete vpc and it automatically deletes subnets and route table.
-------------------------------------
Internet gateway:

When we create a subnet, by default that will be private subnet. i.e devices in this subnet cant talk to internet and vice versa. To make subnets public, we need to attach it to igw. IGW are attached to vpc and they cover all az in that region.
A vpc can have upto 1 igw attached to it. And internet gateway can only be attached to 1 vpc at a time. 
First we need to create IGW and attach it to our vpc,then create custom route table, then configure default route pointing to igw.
When we deploy resources onto public subnet, by default they only get private ip. We need to check box to enable public ip.
anyone who wants to access our resources, will first send request to public ip and then it is converted and forwarded to private ip which will then go to specific ec2 instance/resource. The resources only know about the private ips. The public ip is associated(linked) with private ip.
--------------------------------------
Demo: IGW

Create a Vpc and 1 subnet. Create a ec2 instance and choose this vpc and subnet we created. with security group icmp-ipv4 from 0.0.0.0 . Now when the instance is ready, grab its public ip and try to ping. We can see that it is unable to ping. Even we cannot ssh into it. This is because the subnet we created will be by default private subnet.
go to vpc and create a igw "my-igw". select it ,actions attach to vpc and select ur vpc and attach. But still we wont be able to access our server. 
select your subnet and click on route table tab. we can see there is no default route. we only have local route. For this lets create custom route table (we can also change in custom route table also). 
create route table public-rt --> choose our vpc --> create
click on subnet association tab, edit it and choose our subnet --> save association
Next click on routes tab and edit it and add default route i.e destination is 0.0.0.0/0 and target is the igw we created.
Now we will be able to ssh into our ec2 and ping our server.
-------------------------------
Nat Gateways:

Suppose we have server in private subnet and we need to give it internet connectivity to download and update security patches. we could attach IGW to this but the issue with this is now the server is in public subnet.which means it is open to entire world.
But incase if our servers are meant to be internal server, that only internal team should have access to, we dont want it to be accessed to internet.
We want our server to initiate connection to internet but there should not be any connection to the server from internet. For this we use NAT gateway. 
To use NAT gateway,we still need internet gateway. First we attach igw to vpc and then attach the NAT gateway to public subnet. (we can think NAT gateway as server running on public subnet). Next we need to setup routing so that devices in private subnet will have a default route that points to the Nat gateway. So that now instance will send packet to Nat gateway in public subnet and public subnet will send packet to internet. But we can never initiate a connection from internet to server as our server doesnt have public ip. 
Nat gateway is managed service. In pricing, it will be charged per hour and per GB of data processed.
Nat gateways are not region specific like internet gateways. 
When we deploy nat gateway,we deploy it to a subnet i.e it will be specific to az. if we want redundancy, we need to deploy it to multiple AZ for failover.
Nat gateway uses elastic ips.
Route table of private subnet should point to nat gateway.
A NAT gateway supports 5 Gbps of bandwidth and automatically scales upto 100 Gbps.
--------------------------------
Demo: Nat gateway

Create VPC,1 pvt subnet, create ec2 instance in that pvt subnet. Now our instance is created without public ip. before creating nat gateway, we need to have igw attached to our vpc as wkt we need to deploy nat gateway in public subnet and our subnet will become public after attaching igw. create my-igw and attach it to our vpc. next create public subnet in same vpc. Create 2 route tables, 1 for public subnet and 1 for pvt subnet. So that private subnet will have default route to nat gateway and public subnet will have default route to the igw. public-rt and private-rt . Edit public-rt and add default route to 0.0.0.0/0 to IGW. subnet association, select public subnet and save.
Edit private-rt and in subnet association, select private subnet and save.
click on nat gateway --> create --> my-nat-gateway --> choose your public subnet --> allocate elastic ip --> create.
go to route tables, select private-rt --> routes --> edit route and add 0.0.0.0/0 and choose ur nat gateway --> save
For resilience, you can create nat gateway in another az (for back up).
-----------------------------------
Private and Public subnets:

To determine whether a subnet must be private or public, we need to decide based on that should devices on internet be able to interact with our resources deployed on the subnet. If yes, then it should be public or else private. 
Eg. webserver application should be on the public subnet since users on internet needs to interact with the website, the webserver needs to be on a public subnet.
Suppose we have web application that needs to talk to database, in this case,webserver will be deployed on public subnet and database will be deployed on private subnet as it has sensitive information and only webserver should be able to talk to it.

Private subnet- use case:
Aws should act as an extension for our private data center, then we can deploy all our resources into a private subnet and use a vpn to connect our private datacenter to connect to aws resources.
-----------------------------------
DNS

By default all the private IP addresses assigned to EC2 instance will get a domain name.  
(eg. 10.0.100.10.ec2.internal)This is only on default private IP addresses. So for any resource that wants to talk to our ec2 instance can either send to ip address or its domain name.
By default only private ips get dns entry and public once cant. If we want public ip addresses also to get a domain name, we should enable "enableDnsHostnames" options when we create a vpc. 
enableDnsHostnames option determines whether the VPC supporting assigning public DNS hostnames to instances with public IP addresses.
enableDnsSupport determines whether the vpc supports DNS resolution through the Amazon provided DNS server.
AWS DNS server can be accessed on the second IP of the VPC CIDR block as well - 169.254.169.253
---------------------------------------
Demo: DNS 

Create a custom VPC. now select that vpc and actions --> edit vpc settings --> Under DNS settings, we can see two options "Enable DNS resolution" and "Enable DNS hostnames".
now check the box of "Enable DNS resolution" only and save.
Create a instance in this VPC and enable public ip address and create. Once the instance is created, in details section, we can see ip details. Where in we can see private IP DNS name (ip-10-0-1-144.ec2.internal) but no public IPv4 DNS and this is because we have not checked the box of enable DNS hostnames. And now if we check this box and save, then  
we can see the DNS for this public ip address(i.e ec2-35-173-226-213.compute-1.amazonaws.com) . Now using this DNS name, anyone can access our instance. It will resolve.
Next ssh into this instance to check the other option also. run cat /etc/resolve.conf , we can see the nameserver 10.0.0.2 . Now do --> nslookup google.com and we van see that using AWS dns server 10.0.0.2 , it was successfully able to resolve that. This worked because we have enabled the DNS resolution option in vpc. We can disable this and test it. nslookup wont work. This is because it is trying to send the dns request to that ip address 10.0.0.2 and AWS is now configured not to respond to it. 
------------------------------------
Elastic IP:

We get a public IP for a instance once we create it. but if we stop an start instance, it will change.
Elastic IP is a static IPv4 address. We can allocate it to our account and it becomes reserved for our account and then we can associate this eip with our instance. Now if instance is rebooted or moved to another account, the IP remains the same. 
We can also assign security group to this Elastic IP so that it will be fixed network configuration.
Suppose we want to do some maintanence on our instance,then we can move this elastic ip on another server which is running the same application.
Elastic IP pricing: EIP associated with running instance is zero cost. But if we associate another elastic ip with same instance, then we will be charged with Additional IP charged per hour. 
If we reserve an EIP and dont associate it with instance, it will incur small additional charge a well. 
EIp are specic to a region and cannot be moved to another region. EIP comes from Amazon's pool of ipv4 addresses and also from our custom ipv4 addresses.
----------------------------------
Demo: Elastic IP

create a instance in public subnet. note its public ip, now stop and start it, and we can see ip has changed. Select elastic ip in left side menu --> allocate elastic ip address --> choose region --> amazon pool of ipv4 --> Allocate.
Select this elastic Ip and actions --> associate elastic ip --> choose your instance --> in case if u have multiple private ips, u can choose that ip also --> associate 
ping this ip to test. stop the instance and again start it and now we can see it got the same elastic ip address.
Finally disassociate elastic ip from server and release the elastic ip
----------------------------------
Security groups and NACLs

They both act as firewalls. Firewalls monitor traffic and only allow traffic permitted by a set of predefined rules. They have inbound and outbound rules.
There are two types of firewalls, stateful and stateless.
stateless firewalls must be configured to allow both inbound & outbound traffic.
Stateful firewalls are intelligent enough to understand which request and response are part of the same connection. If a request is permitted,the response is automatically permitted as well in a stateful firewall. 

Network Access Control List (NACL):
NACLs filter traffic entering and leaving a subnet. NACLs do not filter traffic within a subnet. This means two servers within a subnet are allowed to talk to each other. 
NACLs are stateless firewalls,so rules must be set for both inbound and outbound traffic.

Security Groups:
Security Groups act as firewalls for individual resource (EC2,LB,RDS). 
Security groups are stateful,so only the request needs to be allowed. Response is automatically permitted.
Note:
* Security groups, when there are no rules,block everything. And when you add a security group rule,it allows a certain type of traffic.
All security group rules "allow" traffic; there is no "deny" option for security groups.
* NACL rules can either allow or deny traffic. They have Rule number (70,80,90), smaller the number and earlier it will get processed. It also has Allow/Deny option.

Multiple Security Groups:
* You can assign multiple security groups to a single resource.
* The rules for bot sg will get merged.

* By default,sg contain outbound rules that allow all outbound traffic (you can delete this rule)
* Every subnet within a VPC must be associated with a network with a network ACL
* You can associate a network ACL with multiple subnets;however,a subnet can only be associated with only one network ACL at a time.
-------------------------------------
Demo: Security Groups

We can create one sg and the same sg can be used for other resources like lb also. when we create instance, by default it gives access to only one rule is ssh --> port 22 --> anywhere. once instance is created,delete the sg.
Create a new security group "webserver-sg" ,choose vpc ,inbound rule allow ssh traffic.
Now to attach this sg to the instance, select ur instance ,actions --> security --> change sg --> remove the old sg --> choose "webserver-sg" and click add --> save.

ssh into this instance and install a webserver 
--> sudo yum install nginx
--> sudo systemctl start nginx

--> curl localhost (o/p: welcome to nginx)
Now if we grab its public ip and access it in browser, we cant and this is because we need to allow inbound port 80(HTTP) and 443(HTTPS) from anywhere. 
Even if we delete outbound rule, sg is smart enough to allow traffic outside as it is stateful firewall. but from inside our server, if we do ping 8.8.8.8 ,it wont work. so we need to add outbound rule.
We can attach the same sg to multiple servers.
if we want the traffic to come from particular instance to our server, we need to copy that instance pvt ip and paste it in our security group and say allow custom tcp from this ip (OR) we can also add the security group from which we want to allow traffic to this security group. this means "the traffic from any resources which has this sg associated with it is allowed" so by this we dont keep track of IPs of individual resources. 
------------------------------------
Demo: NACLs

Go to your ec2 instances and allow all traffic in sg for testing purpose. Identify the subnet for your intances in the networking tab. Go to VPC and choose Network ACLs in the left menu.select that subnet and we can see in the inbound rules, all traffic rule number 100 is Allow. And another rule below it is All traffic Deny. Traffic doesnt hit deny as it is not having rule number. 
Now edit inbound rules of this subnet and modify rule 100 to allow only ssh from anywhere. by this everything else traffic will hit deny. This is same for another ec2 instance also as they both are in same subnet. but now if we try to access our site in browser, we cant. as we are blocked.
Next go back to NACL and edit that subnet & allow http (rule no 110) and https (rule no 120) and save. Now we can access our site.
Next if we try to install nginx in the server-2 , we cant. This is because of NACl it cant reach internet. Even if we have all traffic allowed in the outbound rules,it cant reach the internet. This is bacause NACLs are Stateless.We need to explicitly mention the outbound rules. Now edit inbound rules again and add rule number 130 and allow all traffic. Now our server will be able to access internet. We need to allow rule on both sides for Nacls.  Now ur site "welcome to nginx" will load fine. now we can remove the all traffic from the inbound rules of the Nacls subnet. 
We can make use of Nacls when we want to filter some traffic .eg. in the same subnet inbound rule, edit it and create rule 90 and deny ssh from 1.0.0.0/24 . so now other then the mentioned ip, all others can do ssh into servers.
This is a benifit of nacls over sg as they can be configured for both allow and deny whereas security group for only allow.   
-----------------------------------
Load Balancers:

Consider we have our app in ec2 intance and user is accessing it. suppose if our instance goes down, then application goes doen and it cant be accessed. to avoid we deploy same application in 3 other servers in different AZ as well. But each of these instances will hae different IPs. What ip address does the user send request to? Instead of letting our user know all ip addresses, we can make use of Load balancers. So now only IP address the user need to know is the ip of load balancer.

Load Balancers in AWS - Types
1. Classic load balancer
2. Application load balancer
3. Network load balancer

Classic load balancer is old generations and has lot of limits. One of the limitations is that we can use only one SSL certificate per classic load balancer. If we have 2 different applications, then we can use only one SSL recommended. hence this lb is not recommended for new application setup.

Application lb works for web based applications and they support HTTP/HTTPS/Web Sockets. And if your application is using some other protocol, you will not be able to use App lb.
They function at application layer (layer 7).
They can forward requests based off of: 
* URL Path conditions,
* Host domain,
* HTTP fields - header,method, query, and IP
* Supports HTTP redirects and customHTTP response
They Perform application-specific health checks

When the client sends request to ALB using any protocol say SSL/TLS, Http/Https are always terminated on ALB.

             SSL/TLS                               HTTP
client  --------------------->   ALB   ---------------------------> EC2 instance
                        SSL certs resides on ALB             (unencrypted)
                       (Encrypted only upto here)

If you still want the traffic to be encrypted between load balancer and EC2 instance,We can make it by adding SSL cert to our EC2 instance as well.We should manage it ourself.

3. Network loadbalancer(NLB):
* Load balance traffic based on TCP/UDP (layer 4)  (It doesnt understands HTTP or HTTPS)
* Meant for applications that don't use HTTP/HTTPS
* Faster than application load balancers
* Health checks are only basic ICMP/TCP connections
* NLB forwards TCP connections to instances. (nothing is terminated here in between)

Since the session is between client and EC2 instances, the traffic is encrypted end to end
            TCP/UDP                             TCP/UDP
client ----------------------->   NLB   ---------------------------> Instance
                       (NLB forward based on UDP/TCP)               (certificate)

They use TLS/SSL for TCP and DTLS for UDP

Network Load Balancer (NLB) can forward HTTP and HTTPS traffic also because these protocols are built on TCP, NLB does not offer features specific to HTTP/HTTPS traffic management, such as SSL/TLS termination or URL-based routing. For applications requiring such features, an Application Load Balancer (ALB) would be a more appropriate choice.

Elastic load balancers work description:
consider we have a vpc and two AZ inside it and our resources are inside it. When we configure ELB here, it will ask for AZ details and we can define this by specifying the subnets.(there are physical resources that get deployed when we are using the load balancer). So we create a subnet in each of the AZ that we want to load balance traffic to and when we select those subnets, AWS will deploy 'LB node' on each of these subnets. And they are responsible for load balancing traffic. Once they are deployed onto the subnets we specify,we can now load balance traffic to any other subnets or even same subnet within that AZ.
When the user or the client wants to send traffic to one of the instance, A DNS record is created for the ELB. And that DNS record is equally forwarded among all load balancer nodes. And taht LB nodes will direct load to respective EC2 instances. 

Cross-Zone Load Balancing:
Consider we have a vpc and two AZ and a ELB and LB node in each AZ. When user sends request to ELB, it will be equally distributed among nodes. And this node will then distribute load among ec2 instance equally (if two instances are there, then 25 % each).
And all these are in same AZ and the traffic cant go accross AZ. And hence Cross-Zone Load Balancing feature was created. It allows LB nodes to load balance traffic to ec2 instances and resources in different AZs.

Load Balancers - Deployment Modes

Public load balancers:
* Deployed on public subnets
* Access by users across the public internet

Private Load Balancers
* Deployed on private subnets
* Access by users within the Organization's AWS Network

Consider you have a Api layer with two instances deployed in public subnet and it has a load balancer attached to it. Here client can send request to these instances through this load balancer as they are in public subnet. Now cosider we have one more layer of database with two instances in private subnet. It is also having a loadbalancer but it is a private load balancer. Now client wont be able to send traffic to this private load balancer from outside the vpc.In the same vpc, the instances or loadbalancers in pubic subnets can forward requests to resources in private subnets. 

In configuring LB, we have two things. Listeners and Target Groups.
Listener is the process that checks for connection requests using the protocol and port that we configure.
there are variety of listeners. We can listen for requests that have specific http methods. If we are using ALB, we can listen for specific hostnames. Any requests that are destined for app1.com we can configure a listener and make it forward it to a target group. 
Target groups route requests to one or more registered targets such as ec2 instances using the protocol and port number that we specify. Even ECS and lambda functions can be configured as target groups.
we may have requests coming for app2.com/auth ,we can forward it to different target groups.which may be ECS. And we may have requests coming for app2.com/cart ,we can forward it to different target groups.which may be lambda functions.
We can also configure health checks on particular target groups. Health checks are performed on all targets regestired to a target group that is specified to a listener in load balancer. Suppose if any instance fails health check, we can stopt routing traffic to that instance.
----------------------------------------
Demo : Load balancer

Create 2 ec2 instances webserver-1 and webserver-2 both created in different AZ us-east-1a and us-east-1b and both are public servers. This is a good design as we have redundancy for AZ failure. And we have two public subnets for them. 
Now lets configure load balancer to balance load across these two instances. Create two extra public subnets for loadbalancer. 
Now come to EC2 page and choose loadbalancer in the left menu. 
create load balancer --> application load balancer --> Name: web-lb --> Internet-facing --> IPv4 --> choose vpc: vpcdemo --> Choose 2 AZ and atleast 1 subnet per AZ : lb-us-east-1a and lb-us-east-1b subnets --> security group --> Listener and routing : HTTP port 80 (but we can change our port to any port number here) --> create a target group --> instances --> name: tg-web --> port HTTP 80 (as our app is listening on default port 80) --> choose vpc --> HTTP1 protocol version --> health checks : HTTP path: / -->  Next --> register targets : select the 2 instances -> port:80 -> Include as pending below -> create target group
Tg is created Now refresh in the app lb creatin page and choose your tg (we can add more listeners here like https) --> create lb.
Once the lb is created, select it and in details we can see DNS name of this lb ,copy it and paste it in browser and we can see our page . we can refresh page to see the page content server1 and server2 .
Now instead of keeping our servers in public subnet, we can keep them in private subnet so that traffic of users will always go through load balancer.only lb should be on public subnet.This gives extra security.
-------------------------------------
VPN (Virtual Private Network):

Assume that we have a vpc which contains a private subnet with certain number of resources deployed onto it. suppose we have a On-premise network and devices in our datcenter needs to talk to the resources in pvt subnet, and we need to be able to safely and securly establish connection between them, then we use VPN here. We deploy the vpn in our vpc which allows the connection between on prem datacenter to the resources in pvt subnet and it will be end to end encrypted.
                                      IPSec
               VPN Gateway -----------------------------> customer gateway
                                  VPN Connection

VPN architecture in AWS:
Consider we have a VPC with CIDR block of 10.0.0.0/16 and we will have pvt subnet and have some resources within those subnet. We need to deploy the VPN gateway and it will terminate VPN on the AWS side. now consider we have on-prem network with CIDR block 192.168.0.0/16 and we need to deploy customer gateway in here. Customer gateway terminates VPN on the Customer side (It can be attachable to one VPC). Both customer gateway and VPN gateway will get public ip address (eg. 1.1.1.1 and 2.2.2.2 respectively). Now we can establish IP sec tunnel between them over the internet.

VPN Routing:
We have 2 types, static method and dynamic method. In static method, in which we manually define a route in the AWS routing table where we specify the IP of the on prem and for this VPN will be pointed and connection will be established between 2 sides.
In dynamic method, We can configure dynamic protocol to exchange routes dynamically using BGP. So now VPN gateway will know how to get to that specific on prem network.

VPN pricing:
* Charged for each available VPN hour.
* Charged for data transfer out from Amazon EC2 to internet.

Note: In AWS, we are charged generally for outbound traffic and not the inbound traffic.

VPN Gateway limits:
* Maximum bandwidth per VPN tunnel of 1.25 Gbps
* Maximum packets per second (140000)
* Maximum transmission unit (MTU) (1466)
--------------------------------------
Direct Connect: (for compliance reason)

It is a physical connection into AWS.An altrnate to VPN. In VPN, The data goes over the internet and internet can be unstable and un reliable hence Direct connect was introduced.
                           Direct Connect
corporate Datacenter --------------------------> AWS resources 

Direct connect Architecture:
consider we have On-Premise Network which has a customer Gateway . now in between this on prem and Aws resources, direct connect location which may be not owned by AWS and they may rent some space here to have routers in there. In this location, We have both customer Router and AWS router. Between them, cross connect is established which is the connection between a port on AWS router and customer router. and now vpn gateway is set up in the AWS for our vpc. and finally aws router will connect with VPN gateway and connection is established. We can also reach public services directly from AWS router and no need for VPN. 
Direct connect will be faster and relible and low latency.

Direct Connect pricing:
* charged for Port Hours
* charged for Outbound data transfer.
---------------------------------------
VPC peering:

* By default resources in two different vpc cannot talk to each other since VPC acts as a network boundary. If we want to establish connection between them, we need to use VPC peering.
VPC peering is the network connection between two vpc and it handles routing traffic between vpcs.
With VPC peering, we can setup connection between VPCs in same region and vpcs in different region and also VPCs in different AWS accounts.  

VPC peering pricing:
* NO cost for VPC peering connection creation. 
* Data transfered within an AZ via VPC peering is free.
* Data transfer across VPC Peering between AZ incurs charges. 

How does it work?
Consider we have VPC 1 with CIDR 10.1.0.0/16 and VPC 2 with CIDR 10.2.0.0/16 . One VPC will send request to another for initiating peering. so the owner of VPC 1 will send peering request to owner of VPC 2. Once the request is sent, the owner-2 will except the peering request. Now peering is established between these VPCs. Next we need to handle the routing.this is to tell our VPC one about how the get to the 10.2.0.0/16 CIDR block. For this we need to create the route table where we define the CIDR of VPC 2 and target where we specify unique identifier of our Peering. So that any traffic from VPC 1 to VPC 2 will match this route and it'll notice send it across the peering.
We need to do the same thing in the VPC 2 also by adding the CIDR of VPC 1 in routes.Now the instances in two different vpcs can communicate freely. 

In case if we have 3 VPCs,and we have peering between VPC 1 to VPC 2 and VPC 2 to VPC 3.
Now VPC 1 cannot talk to VPC 3 using VPC 2 a transitive. If we want VPC 1 and VPC 3 to talk to each other then we need to setup their own VPC peering as well.
---------------------------------------
Demo: VPC peering

Create two VPCs VPC-A with CIDR 10.1.0.0/16 and VPC-B with CIDR 10.2.0.0/16. Create Server-1 in VPC-A and Server-2 in VPC-B. Login to Server-1 and try to ping the pvt ip of server-2.Ping is not successful. 
Lets setup the VPC peering. Go to Peering connections in left menu in VPC --> create peering connection --> name: vpca to vpcb --> Requester: VPC-A --> Select another account for VPC to peer with (My account and this region) -->  Accepter VPC ID : VPC-B --> create. 
Now in the peering connections, we have a "pending acceptance" status. Select it and actions --> Accept request. Now peering is successful.  But still we will be unable to ping the ip of server-2. We need to set up route between them. 
Go to Route tables,select route table of VPC-A ,in the routes tab, we can see local route and default routes(igw) only.   
Edit routes and in destination add 10.2.0.0/16 and in target choose peering connection: vpca to vpcb --> save
Now VPC-A know about CIDR of VPC-B but still VPC-B doesnt know about VPC-A.
In VPC-B add routes of 10.1.0.0/16 and choose vpc peering in target --> save
Now try to do ping and ping is successful.Its all going through AWS infrastructure only and not the internet. 
--------------------------------------
Transit Gateway:

WKT VPCs cant talk to eachother by default. To allow this,we need to setup vpc peering connection between them. As the number of VPC increases, it will become difficult to manage them by peering them each other. And if we have 4 VPC and we need to connect to them from on prem, then we need to setup vpn connection from customer gateway to all 4 VPC seperately. By this number of VPN will also increases. To address this issue, a service called Transit Gateway was created.
The idea behinf Transit gateway is to avoid creating full mesh of VPCs and having to maintain all of them. 
* Transit gateway acts as transitive routing device to route traffic between VPCs.
* We must specify 1 subnet for each AZ to be used bt the transit gateway to route traffic. 
Now consider we have 4 VPCs and we deploy a transit gateway. and these 4 VPC only need to peer or connect with transit gateway and no ned to peer with one another. And the transit gateway acts as a router that routes traffic between VPCs.
With Transit Gateway, we no need 4 VPN connections, instead just connect the on prem customer gateway with Transit gateway. In same way we can also utilize direct connect.
one Transit gateway can peer with other Transit gateway within same region or also between one region to another region and also transit gateway in different AWS account.
---------------------------------
Private link:
Consider we have EC2 instance deployed in pvt subnet. And if this instance need to access S3 bucket, then we need to provide access to internet by creating NAT gateway or internet gatway. By doing this our EC2 instance will be exposed to internet which we dont want. This is where Private links are used.
Private links give your VPCs direct access to the public services like s3,lambda,cloudwatch etc so the traffic doesnt need to route through the internet.and our instance is more secure now. 
Private links also allow the resources in one VPC to connect with services in another VPC using pvt ip addresses. Suppose if a third party company was providing some service and we need access to it,then we can create private link to the VPC that service runs into and give direct access as if it was hosted directly in our own VPC
------------------------------------
CloudFront:

In the concept of global content delivery and edge locations, consider we have a webserver in N.America and if a user in N.America can access the applications without any latency but for the user in India, there will be high latency due to long distance.
To solve this issue,AWS provides Edge locations. AWS has smaller edge locations scattered accross all globe. our web application files will be cached in these edge locations. Now the user from remote locations can send request to these edge locations instead of long distance webservers.This reduces latency. Cloudfront is basically a CDN. 

Cloudfront is a web service that speeds up distribution of static as well as dynamic contents such as HTML,CSS,JS,media files like images,videos,song etc. so that users get access to them very fast. 
When the user sends requests to cloudfront, their requests will be routed to Edge location first and they provide lowest latency or time delay so that content is delivered with best possible performance.

Cloud front architecture:
There is 'origin'. Origin is the source location for content that will be cached by cloudfront. Eg. images and files stored in S3 bucket is a origin. And cloudfront will take these files from origins and cache them in the edge locations.
Now suppose we are configuring cloudfront, after origin,we need to configure distribution. Distribution is a configuration unit/block in cloudFront. In Distribution we tell cloudfront where we can find origin/source files, and cloudfront will create a dynamic domain name for us (eg. http://xyz.cloudfront.net). We can now access cached images at edge locations by using this domain. 
If a user sends request to Edge location and if edge location doesnt have that content,then edge location will forward request to origin and get the content and display it to user.And now that content will be cached at edge location for fast retrieval.  
cloudfront TTL:
* Cached content at edge location remains for a set time known as time to live (TTL). 
* TTL value decides content validity before an edge location requests the origin.
* Default TTL is 24 hours.
* Can be configured to have objects expire at a specific time. 

Cache invalidation allows you to invalidate content cached at edge locations.
suppose we have the version 1 of the file in origin and it is cached at edge location and now we have updated that file to version2 in the origin but the edge location still have version1 of that file and it will stay there till 24 hrs and the users will see the cached content only. To address this issue, we do cache invalidation which will remove all cacheed content in edge locations. and now if the user sends the request to the edge location, those files wont be there and it will send request back to origin and gets the new version2 file. Cache invalidation is done before the TTL expires.

Cache Invalidations:
* Invalidations are performed on a distribution. (we invalidate individual distributions)
* You can invalidate all objects in a distribution,a specific folder,or a specific object. 
* /* - Entire disribution invalidate.
* /file.txt - individual file.
* /images/* - All objects in images directory

CloudFront - Basic integrations with Other Services.
SSL/TLS is enabled by default and default domain name is https://xyz.cloudfront.net and AWS will provide default SSL certificate *.cloudfront.net and we can also setup our custom domain name: https://kodekloud.com where we can utilie AWS ACM and we can create certificate for our cloudFront distributions by using custom certificate.
* Cloudfront will Automatically publishes operational metrics for distributions. 
* We can enable extra metrics for additional cost as well. 

Use cases:
We can cache Static websites,video on demand and also for streaming
--------------------------------------
Demo: cloud front

In AWS , go to S3 and create bucket with publically accessible. Upload a image to bucket. In the bucket policy, add the policy for s3:GetObject and save it. now we can access the file using object URL. Go to cloudfront and for origin domain choose your S3 bucket url --> In origin path we can choose the particular path if we have subfolders in our bucket (eg. /images) or else leave option empty --> name --> origin access : "Public" (choosed) (if users dont want to use cloudfront distribution,they can access s3 bucket directly for which bucket must be public) (or) "origin access control settings" (bucket can restrict access only to cloudfront) --> Default cache behaviour : Compress objects automatically: yes --> Protocol HTTP and HTTPS (or) Https only --> Allowed methods: Get,Head,put,post --> Web Application Firewall (WAF): Do not enable security protection --> use all edge locations --> if u have certificate,u can choose here --> create distribution
Now our distribution is created. select it and we can see the Domain name. And now if we access this url, it wont load because we need to mention the name of the image as well along with this domain as a context path.This  domain name is accessing the edge location and not the S3 bucket. Now if we delete this image from s3 bucket,and upload another image with same name. But if we access the cloudfront domain,we still se the old image. This is because image is cached and it exists untill the ttl expires.
Go to cloudfront distribution and in the "Behaviours" tab, and there will be default which is 86400 sec(1 day). so we need to do invalidation now.
go to the "invalidations" tab --> create invalidation --> Add object path : /* --> create invalidation
Now if we access the distribution domain, we can see the new image. 
------------------------------------
Lambda@Edge

Cloudfront functions and Lambda@edge:
Using this feature, we can write light weight functions that can run at these edge locations and these functions can perform variety of operations like manipulating the requests and responses that flow through cloud front, also they can perform basic authentatication,authorization as well as generate responses at the edge so that we dont need a physical server now so that all the logic happens at edge location itself.

When do functions Run?
Cloud front functions:
* Cloudfront funcs will run when cloudfront receives a request from a viewer (viewer request).
* Before Cloud front returns the response to the viewer (viewer respose) 

Lambda@edge functions:
* When cloudfront receives a request from a viewer (viewer request)
* Before cloudfront forwards a request to the origin (origin request)
* When cloudfront receives a response from the origin (origin reponse)
* Before cloudfront returns the response to the viewer (viewer response)

Cloudfront functions use cases: (for running small functions)
1. Cache key normalization (transform http request attributes to create optimal cache key which improves caching ratio)
2. Header manipulation (we can insert,modify,delete http headers)
3. Url redirects or rewrites
4. Request authorization (validate Json web tokens by inspecting authorization headers)

Lambda@ege use cases:
* Long-running functions
* configurable CPU and memory functions
* Dependencies on third-party libraries
* Network-dependent functions 
* File system or HTTP Request Access functions
------------------------------------------
Global Accelerator: (for optimizing paths)
Consider our application deployed in N.America and user access it through remote locations, then the request will go through the internet and they have inefficient path, they are not reliable as well and latency will also be more. Global accelerator will help address this issue so that we can improve the experience of end users.
When we enable Global accelerator, we have access to all their edge location. These edge locations are different compared to cloudfront edge locations. Now when a user sends the request so they can talk to our application,it will reach the closest global accelerator edge location. By this user will no need to go through regular internet and instead they go through AWS backbone network. So now it will be much faster connection and secure.
AWS backbone is lot faster compared to other regular internet providers. 
--------------------------------------------
Route53:

* An AWS managed DNS service 
* Acts as a domain registrar - purchase domains similar to sites like Godaddy and Namescheap (eg. www.kodekloud.com)
* A global service - not specific to a region

Hosted zones: collection of DNS records for specic domains.
Our domain name will be created using hosted zone. We define all of our records and rules in Hosted zone. When we create hosted zones for specific domains, AWS will reserve/allocate 4 nameservers to hosted zones.
-------------------------------------
Demo: Route53

Go to rout53 --> domains --> registered domains --> enter your domain and checkits availability : kokeklouddemo123.com -> select it --> proceed to checkout --> check box of auto-renew --> next --> provide you contact details --> Verify (we also incur small charge for hosted zone) --> Submit
Wait for some hours and now we can see our domain name in the list.And we can see the 4 nameservers allocated to this. Go to hostedzone and we can see one hosted zone. And in that we can see dns records configured automatically. We can create more DNS records by clicking on create record --> A record (to point to ip of our webserver) --> Recordname: www or leave it empty (subdomain) and paste the ip in the value --> create record
view status and wait till u see INSYNC.
Now go to browser and enter 'kodeklouddemo123.com' and we can see our web page. 
---------------------------------------
Route 53 Application Recovery controller
It is a service that contineously monitors applications ability to recover from failure. And to control app recovery accross multiple AWS regions and AZ. 

Consider we have application deployed in one region and a stand by app deployed in another region. we can use Application Recovery Controller to monitor backup environment. Make sure its up and running,scaled up and configured properly so that we know that when the time comes that do we need to failover to it, it will be up and running. Based on our previous configuration, we can have Application Recovery Controller to monitor our primary deployment. If any issue in our primary app, then we can failover to backup site. We can divert all trffic to backup site untill we figure out the issue in primary site. 
We can also use ARC to manually diversion of traffic. 

Application Recovery Controller Architecture:
Consider we have our active application in us-east-1 region with lb and ec2 (this full this is cell-1) using a global dynamo db (resource set) and we have a failover setup in us-west-1 region with same setup (cell-2). We have route53 on top of these which sends traffic to active region and if any issue, we can modify in route53 to send traffic to standby region.
* cell groups all resources required for an application to operate independently. 
* Recovery group is a collection of cells that represent an application that i want to check for failure readiness. It monitors both active and standby site.
* Resource set is a set of AWS resources that can span multiple cells. 
We can setup Application Recovery Controller to do readiness checks to monitor the readiness of the standby deployment.
* Routing Control allows you to manually failover entire application stack to standby site. And finally Route53 will do the failover.
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Services: Storage

Elastic Block storage (EBS):
Block storage breaks up data into blocks and then store these blocks as seperate pieces each with a unique identifier. And these blocks can then we stored across number of physical devices. A collection of blocks can be presented to you OS as a volume.Then we can create File system on top of it. We can also present it as a hard drive which allows us to install OS on it to boot up.
* So Block storage is both mountable and bootable. (IMP)  
AWS Elastic block storage (EBS) is the same.
We can attach EBS to an instance,detach it and attach it to another instance. And all data remains intact.
Normally EBS volumes can be attached to a single ec2 instance whereas certain volumes can allow multi attach where multiple instances can attach to same volumes. Where our application must be intelligent enough to not have multiple ec2 instances write to the same data at same time and this may lead to curropt the data. 
Eg. DB clusters will be intelligent enough to have master node responsible for writing to data and worker nodes just read from it. 

EBS volumes must be provisioned in an AZ.Redundancy is built in within the AZ. It can handle physical device going down but data is lost if entire AZ goes down. 
To attach ebs volume to ec2 instance, both must be in same AZ. But incase if we want the data in the volume of one AZ to be available in another AZ,then there is a feature called "snapshot". We create a snapshot of the volume in AZ1 and from this snapshot we can create new volumes in another AZ2.And then we can attach this newly created volume to EC2 instance.Snapshots can be stored in S3. 
Next to get the data between regions, again we can take snapshot of ebs volume in region1 and keep it in s3 bucket and then we can copy the snapshot to another s3 bucket in another region and finally create volume out of this snapshot.

EBS volume types: 
1. General purpose SSD [gp2 & gp3] 
2. Provisioned IOPS SSD volumes
3. Throughput Optimized HDD volumes.
4. cold HDD volumes
5. Magnetic

General purpose SSD [gp2 & gp3] :  {max 16TiB vol size, 16,000 iops per volume, 1000 MiB/s max throughput per volume(gp3)}
These are volumes backed by SSD (Solid state Drives). They give better pricing and performance. we can use them for wide variety of workloads like virtual desktops,medium size,single instance dbs,latency sensitive interactive applications, as well as dev and test environments. We can startoff with SSD. They dont support multi attach.
There are 2 types gp3 and gp2 volumes.
* gp3 volumes are latest generation SSD volumes and lowest cost volumes offered by aws.  
this Volume type provides right balance of price and performance for most apps.It aso helps scale volumes performance indepedently of volume size.gp3 offers 20% lower price compared gp2 volumes. 
* gp2 volumes are default volume types for ec2 instances. They offer cost effective storage ideal for broad range of transactional workloads.The performance scales with volume size.

Provisioned IOPS SSD volumes (io1 and io2):
They are backed by SSD (Solid state Drives)
These are the heighest performance EBS storage volumes. They are designed for critical,IOPS intensive,Throughput-Intensive workloads.
The different versions available are,
1. Provisioned IOPS SSD (io2) volumes   {max 16TiB vol size, 64,000 iops, 1000 MiB/s max throughput per volume}
2. Provisioned IOPS SSD (io2) Block Express volumes      {max 64TiB volsize, 256000 iops , 4000 MiB/s max }
3. Provisioned IOPS SSD (io1) volumes.

io2 has more durability compared to io1 volumes (99.999% durability). They support multi attach. used for high intensity workloads like db workloads needs more performance.

Throughput-Optimized HDD and Cold HDD volumes.
They are traditional HDD based volumes and they can be slower. Thre are 2 types.
1. Throughput-optimized HDD : Low cost HDD designed for frequently accessed throughput intensive workloads
2. Cold HDD: It is the lowest cost HDD designed for less frequently accessed. (cheapest)

Throughput-optimized HDD {500 iops/vol, 500 mib/s max throughput per vol}
Cold HDD {250 iops/vol, 250 mib/s max throughput per vol}
max vol size for both 16 TiB

Magnetic volumes:
They are previous genertions of volumes backed by magnetic drives. Suaitable for dataset,infrequent data access and performance is not of primary importance.Max iops of 200 and vol size upto 1 TiB

EBS pricing:
We are charged per GB per Month. Cost also depends on the volume type selected. Faster IOPS more cost. Snapshots are also charged per GB per month. This is for full snapshot and not for incremental snapshot ,we are paying full cost. 

The thing that seperates block storage is that we can both Boot and mount block storage unlike file system storage and object storage.(IMP for exam)
------------------------------------------
Demo: EBS Part1

Create 3 servers in us-east-1 region , in which server-1 and server-2 in us-east-1a AZ & server-3 in us-east-1b AZ and server-4 in us-west-2 region
Go to volumes and create volume --> gp2 --> 10 GB --> AZ : us-east-1a --> enable encryption --> create
Give a name: demo-volume ,it will be in available state. select it and actions --> attach volume --> choose instance :server-1 --> devise name : /dev/sdf (but it can also be /dev/xvdf thru /dev/xvdp) --> attach
Login to server1 and run lsblk to list our all the volumes attached to this. We can see output xvda and xvdf. xvda is the root / volume that we got when we created our instance. the sdf got renamed to xvdf as per the note while attaching volume. First we need to check if there is any file system in the attached volume 
--> sudo file -s /dev/xvdf       (o/p: /dev/xvdf: data  --> indicates that there isn't any file system currently)
Now lets create a file system on this volume
--> sudo mkfs -t xfs /dev/xvdf
Now the file system is created. Now re run the previous command to see the fs.
Now lets mount this fs in a directory 
--> cd /
--> sudo mkdir ebsdemo
--> sudo mount /dev/xvdf ebsdemo 
--> df -kh       (to see the mount)
This is a temporary procedure to mount and if we reboot the volume, it will no longer be mounted. In order to make this permanent,we need to configure fstab for which first we need to get the unique identifier for our ebs volume.
--> sudo blkid  (o/p: /dev/xvdf: UUID="045ddc53-453-986c") and copy it
--> sudo vi /etc/fstab
We can notice how other mounts are setup and add a new line 
UUID=045ddc53-453-986c  /ebsdemo  xfs  defaults,nofail  (add this and save file)
--> sudo mount -a  (this will automatically mount whatever listed in the fstab file)
--> cd /ebsdemo/
--> sudo echo "this is test comment on server1" > demo.txt

Now lets detach this volume from this instance and attach it to another instance
--> cd /
--> sudo umount /ebsdemo 
--> df -kh  (verify it)
go to volumes select it and actions --> detach volume. again select it and attach it to server-2 (in same AZ). login to server-2 now and verify if it is attached.
--> lsblk   (/dev/xvdf)
--> sudo file -s /dev/xvdf  (we can see the file system because we created it in another machine)
--> cd /
--> mkdir ebsdemo
--> sudo mount /dev/xvdf /ebsdemo
--> cd ebsdemo
--> ls   (o/p: demo.txt)

Like this we can perform these steps if we are performing maintanance or migrating our data between instances.
------------------------------------
Demo EBS : Part-2

Now lets move our ebs volume to server-3 which is in different AZ. 
Unmount your ebs volume first
--> sudo umount /ebsdemo
--> df -kh
Detach it . now select volume again and we need to create snapshot --> name: my-snapshot --> create snapshot
once it is availabe, select that snapshot --> actions --> create volume from snapshot --> choose AZ: us-east-1b --> encrypt --> tag --> create.
our volume is now created in us-east-1b and now we can select it and attach it to server-3 on /dev/sdf --> attach
login to server-3 and lsblk
--> sudo file -s /dev/xvdf  (o/p: xfs filesystem is present)
--> sudo mkdir /ebsdemo
--> sudo mount /dev/xvdf /ebsdemo
--> cd /ebsdemo/
--> cat demo.txt

Next inorder to copy the volume to server-4 in different region, we cannot follow the same steps as before. For this we need to copy it to s3 bucket or to desired region.
Select the snapshot --> actions : copy snapshot --> description: copy of my snapshot -->Destination region : us-west-2 --> copy snapshot
Now we can go to us-west-2 and see the copy of our snapshot.select it and create a volume in this region in same AZ as our instance and finally attach it to server-4. Follow the remaining steps from above
----------------------------------
Instance store

Instance store provides temporary block level storage for our ec2 instances. Depending on ec2 instance type, AWS may provide instance store which is temporary block level storage.
Instance store are located physically on disks attached to the host computer. Unlike EBS volumes, where ebs data is stored on seperate m/c and we connect to it remotely, Instane store is physically on host computer thst your EC2 instance run.
Instance store is only meant for temporary storage or information that changes frequently like tempory content or scratch data. 

Instance storage Architecture:
consider we have 2 ec2 instances running on our host machine. as the instance store are physical disks that are on the host machine, so when our EC2 instance go down ,and come back online it will still have access to instance store. Even if it is temporary,if the ec2 instance is on same host, then it will always have access to same instance store. 
Problem arises when te EC2 instance goes down and come back on another host-2. So now it wont be able to access that instnce store as it recide on completely seperate host. But instead The new ec2 on another host will have access to instance store in that particular host. But all of the data it had on instance store 1 is now gone. It will be starting with a blang state on instance store 3. Thats why it is called temporary storage.
---------------------------------------
Demo: Instance store Volumes

go to EC2 --> launch instance --> not all ec2 instance type support instance store so choose m5dn.large --> choose key pair --> under volume section, we get instance store volume,we get volume2 with size 75GB and device name /dev/nvme0n1 --> enable public ip --> launch instance
Login to the instance and do lsblk to see the attached volumes (o/p: we can see / and nvme1n1)
--> sudo file -s /dev/nvme1n1
--> sudo mkfs -t xfs /dev/nvme1n1
--> sudo mkdir /instance-demo
--> sudo mount /dev/nvme1n1 /instance-demo
--> df -kh
--> cd /instance-demo
create a file with text
Now ec2 instance moving to other physical host doesnt mean that after reeboot the instance will move to otherhost,it will still be in same host. 
Test it by rebooting but you can still see the same public ip for the instance. Thats how we know if our ec2 has moved to different physical host or not. 
Again login to instance and run mount command again and we can see the file we created.

This time stop the instance and later start it and the public ip will change. Now our instance has moved to different host. Now it is going to get the brand new instance store volume as old one was in the other physical host. Now login to this instance
--> lsblk  (o/p: nvme1n1 is still attached)
--> sudo file -s /dev/nvme1n1  (o/p: /dev/nvme1n1: data)
This means all the data is gone as there is no file system

Remember that instance store volumes are not ment for data that persists. It is only for temporary data.
--------------------------------------------
EFS (Elastic File System)

There are 2 types of file system provided by AWS
1. AWS EFS
2. AWS FSx

EFS uses network file system v4 protocol NFSv4. Any application using this protocol can work with AWS EFS. With EFS we create a file system and EC2 instances and other computer services can remotely connect to that file system and mount them.
EFS only works with only linux. With EFS file system, data can be shared across multiple EC2 instances at the same time.
To create EFS , we deploy it onto VPC (EFS is VPC specific). Then inside VPC,it is made available to AZ using mount target and once they are deployed into subnets they get IP address. eg. 10.0.1.10  
EC2 instances can then connect to the file system by specifying the IP address of the mount target. For high availability, we can have mount targets in multiple az (eg. 10.0.2.10 in az2).  
EFS offers following storage classes. 
1. Standard storage classes like EFS Standard and EFS standard-Infrequent Access (Standard-IA). They have multi-AZ resilience and the highest levels of durability and availability.  
2. One zone storage classes like EFS One-zone and EFS One Zone-Infrequent Access (EFS One Zone-IA). The choice of additional savings by choosing to save your data in a single Availability Zone.

EFS also provides different performance modes that handle different throughput,Iops,low latency needed for a broad range of workloads.
1. General Purpose Performance Mode: suitable for latancy-senstive applications like web-serving environments,content-management systems,Home directories,General file serving.
2. Elastic throughput Mode: Automatically Scale Throughput performance up or down to meet the needs of your workload activity.
3. Max I/O Performance Mode: Higher levels of aggregate throughput and operations per second. They have higher latency for file system operations. 
4. Provisioned throughput mode: Level of throughput the file system can drive independent of the file system's size or burst credit balance. 
5. Bursting throughput mode: Scales with the amount of storage in your file system and supports bursting to higher levels for up to 12 hours per day. 

Installing amazon-efs-utils
--> sudo yum install amazon-efs-utils -y
--> sudo mount.efs efs:id /directory
The efs:id is the ID of the efs filesystem and can be found in the AWS console. /directory-select which directory you would like to mount the filesystem at
 
EFS can be mounted but cannot be booted, can't install an operting system. This is the difference between EFS and EBS volumes
-------------------------------------
Demo: EFS

Create 2 instances in 2 different AZs US-east-1a and US-east-1b . Now lets create a managed EFS and mount it on to both of these servers. They can both be reading and writing to it at the same time. 
Goto EFS --> create file system --> customize --> name: efsdemo --> standard --> Enable automatic backup --> life cycle management -> tarnsition into IA: 30 days since last access --> enable encryption at rest --> throughput mode: Enhanced --> Next --> choose vpc --> select AZs and our efs security group we created (to allow traffic from the sg of our instances) and it will automatically configure mount targets (it is good to have mount targets in all AZ for redundancy) --> create olicy if u want --> create
Now our EFS file system is ready and login to both servers, 
--> sudo mkdir /efsdemo   (in both servers)
--> sudo yum install amazon-efs-utils -y (in both servers)
--> sudo mount.efs fs-08d374b450987 /efsdemo    (the efs id we get from efs console) (in both servers)
--> df -kh 
--> cd /efsdemo
--> sudo vi file1
"server1 text"
--> go to server2 and we can see the file1 there in /efsdemo path
Similarly you can create file2 in this server and check it in server-1

You can also configure fstab for this
----------------------------------------
FSx:

FSx is a fully managed service that provides high performance file storage for wide range of workloads designed to easier for businesses and organizations.
EFS only works with linux. If we have windows based m/c,we should use FSx. 

FSx will provide below features:
* Provisioning file servers and storage volumes
* Replicaing data
* Patching file server
* Addressing hardware issues
* Performing manual backups.

Fsx-Benefits
* storage (store files,docs,photos,videos)
* managed
* Scalable
* shared access
* Backups

Different Flavors of FSx
1. Amazon FSx for Windows file server.
   * It supports the server message Block (SMB) protocol
   * You can easily integrate it with Microsoft Active Directory
   * It supports data deduplication
   * You can set quotas

2. Amazon FSx for lusture.
   * optimized for high performance.
   * It provides low-latency,high-throughput access to data. 
   * It is built on the lusture file system
   * Amazon FSx for lusture integrates seamlessly with other AWS services like Amazon S3,AWS DataSync, and AWS Batch
   * You can easily scale the file system's apacity and throughput

3. Amazon FSx for NetAPP ONTAP.
   * It offers high-performance stprage that's accessible from linux,windows,and macOS via NFS,SMB,and iSCSI protocols
   * It can scale your file system up or down in response to workloads demands
   * It can perform snapshots,lones,Replications and much more

4. Amazon FSx for openZFS
   * It is built on top of the open-source OpenZFS file system
   * It supports access from Linux,windows,and macos via industry standard NFS Protocol
   * It utilizes power openZFS capabilities including data compression,snapshots,and data clonning
   * It offers built-in data protection and security features.

Deployment options for FSx:
* FSx for windows,OnTap,OpenZFS supports both single-AZ and Multi-AZ deployments
* FSx luster supports only single AZ deployment.
-----------------------------------------
S3 overview:
 
