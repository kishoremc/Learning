Kode Kloud solution Architect notes:

**** Services: Networking

VPC: (Virtual Private Cloud)
It is a secured,isolated network segment hosted within AWS.
They allow us to isolate resources from other resources in the cloud. (If we dont want applications to talk to each other)
With VPCs, we get access to subneting (IP Address),Routing (Route tables),Firewalls (NACLS and Security Groups) & Gateways.
VPCs are specific to a single region. (eg: vpc1 in us-east-1 region and vpc2 in us-east-2 region)
VPC acts as network boundary. And resources created in these vpc are completly isolated. It is also a kind of security. And if we want connection between them, we need to explicitly allow it.

Every VPC has a range of IP addresses assigned to it called CIDR block.
A CIDR block defines the IP addresses that resources in the VPC can use.
(Eg. If we have the server deployed in VPC-1, it can only get IP address within that CIDR block)

A CIDR block size can be anywhere from a /16 to a /28 .
Assume we create a VPC-1 and it has a CIDR block of 192.168.0.0/16. This means the IP addresses from 192.168.0.0 to 192.168.255.255 are avilable for our resources deployed in VPC-1.
We can also enable optional secondary IPv4 Block. and Optional IPv6 /56 CIDR Block. And can have upto 5 IPv6 CIDR blocks, but this limit is adjustable.

There are 2 types of VPCs:
1.Default VPC 
2.Custom VPC

Default VPC is automatically created by AWS in all the region by default. the services deployed in it can automatically access internet.
They always have /16 IPv4 CIDR block 172.31.0.0/16 (65,536 addresses). This is common for all regions and all accounts. We get 1 default subnet (/20) in each Availability Zone (4096 addresses). (Eg. 172.31.16.0/20 in AZ1 and 172.31.32.0/20 in AZ2).
By defaultly Internet gateway attached to the VPC. A route that points all traffic (0.0.0.0/0) to the internet gateway. Devices in these default subnets will be accessible from the internet. Default security group (allowing outbound traffic by default) and Default Nacl (allows both inbound and outbound traffic).
The Default VPC and its subnets have outbound access to the internet by default.

In custom VPC, We need to do all the settings in it.
-----------------------------------
Demo: Custom VPC

Goto AWS,select region and go to VPC. We can see default VPC here. Create vpc button
Choose option VPC only (or vpc and more).
demo-vpc --> Ipv4 CIDR block : 10.0.0.0/16 --> no IPv6 CIDR block --> add tags --> create vpc . We can see all the details of our VPC now. We can also see "Resource map" tab for it gives complete information about our VPC.
Select the VPC and click on delete.
------------------------------------
Demo: Default VPC

Go to VPC and click on the default VPC to see more information of it.
Go to subnets tab and we can see 6 default subnets in us-east-1 (N.Virginia) region. This is because we have 6 AZ in this region. Now we can go to resource map tab and visualize it. we can also see 1 default route table and 1 igw (provides internet connection to the resources attached to it) 
Since we have igw associated with this VPC, the "Auto-assign public IPv4 address" option is set to 'Yes'. This means when we deploy a server in this VPC, its going to automatically get public address and we will be automatically able to connect to it.
You can test this by launching EC2 instance in default subnet.
---------------------------------
Subnets:

Subnets are groups of IP addresses in your VPC.
A subnets resides within a single Availability Zone.
Subnets can be made public or private to allow external access to resources within them. 
Subnets within a VPC must be within the CIDR range. (if our VPC CIDR is 192.168.0.0/16, then subnet of 192.168.10.0/24 is valid but 10.100.1.0/24 is invalid as it is not within the VPC CIDR range.)
A subnet block size must be between a /16 and a /28 .
The first 4 IP addresses of a subnet are reserved and cannot be used.
i.e 192.168.10.0 (Network address), 192.168.10.1 (VPC Router), 192.168.10.2 (DNS), 192.168.10.3 (for Future Use), and last IP address for subnt (192.168.10.255) is reserved for broadcast address.

Subnet configuration options:
* Subnets cannot overlap with other subnets in the VPC 
Eg:
Subnet A: 10.16.0.0/24 and Subnet B: 10.16.0.128/25 in the same vpc is invalid
Overlapping IP Address Ranges,
Subnet A: 10.16.0.0 to 10.16.0.255
Subnet B: 10.16.0.128 to 10.16.0.255

* A subnet allows for an optional IPv6 CIDR.
* A subnet can be configured to be IPv6 only - No IPv4 addresses
* Subnets can communicate with other subnets in the VPC
* Auto-assign public IPv4/IPv6 address in addition to the private address.
--------------------------------------
Demo: Subnets

Create a new custom VPC with CIDR 10.0.0.0/16 with Amazon provided ipv6 CIDR block.
Go to subnets --> select VPC --> subnet-1 --> AZ: us-east-1d --> IPV4 CIDR block: 10.0.1.0/24  --> IPc6 CIDR block : 2600:1f18:24e0:de00::/56 --> create subnets.
Similarly create one more subnet 10.0.5.0/24 in us-east-1a region. 
Now if we want to deploy our server in subnet-1, then the server will be deployed in us-east-1d. 
Now we can test this by creating an instance in this subnet. Now we can see that the instance private ip is 10.0.5.113 which is within the subnet we specified.
Now delete the instance and delete the VPC (This will automatically delete the subnets in it).
-------------------------------------
Routing in VPC:

Every VPC has a VPC router. And this router can be accessible from a network and 1 address of each subnet. Purpose of router is to route traffic between subnets and also in and out of vpc. 
A route table is set of rules that the router uses to forward network traffic.   
Each rule in the route table is referred to as 'route'.
All route tables have one route by default i.e local route.
The router has an interface in every subnet of VPC and is reachable from the network + 1 address of each subnet. 
Every subnet is associated with route table. We can specify the route table with which the subnet is associated with. 
Suppose we have subnet in AZ1, then any traffic leaving this subnet will follow the rules in the route table and decide where the package should be sent.
When we create vpc, it will automatically create default route table for us. and the new subnets created will automatically going to get associated with this route table untill we point it to different route table.
Multiple subnets can be asociated with single route table. But a subnet can be associated with only one route table at a time. (IMP)
---------------------------------------
Demo: Route table

Create a vpc and 2 subnets. Now we can see our subnets are automatically assigned to a main route table.click on the route table.
Now we can see in the subnet association, there is no explicit subnet association and there we can see only subnet without explicit association.
Create our own route table. create routr table --> name--> choose vpc --> create
Now click on edit subnet association and choose the subnet-1 .
Now this means that any traffic that comes from subnet1 will follow the rules associated with route table 1 (rt1). We can se it in the routes tab. Similarly create one more rt2 and associate subnet 2 with it.
Now click on routes tab and edit route ,add route 0.0.0.0/0 and choose whatever target u want i.e nat,local,igw etc and save
Finally delete vpc and it automatically deletes subnets and route table.
-------------------------------------
Internet gateway:

When we create a subnet, by default that will be private subnet. i.e devices in this subnet cant talk to internet and vice versa. To make subnets public, we need to attach it to igw. IGW are attached to vpc and they cover all az in that region.
A vpc can have upto 1 igw attached to it. And internet gateway can only be attached to 1 vpc at a time. 
First we need to create IGW and attach it to our vpc,then create custom route table, then configure default route pointing to igw.
When we deploy resources onto public subnet, by default they only get private ip. We need to check box to enable public ip.
anyone who wants to access our resources, will first send request to public ip and then it is converted and forwarded to private ip which will then go to specific ec2 instance/resource. The resources only know about the private ips. The public ip is associated(linked) with private ip.
--------------------------------------
Demo: IGW

Create a Vpc and 1 subnet. Create a ec2 instance and choose this vpc and subnet we created. with security group icmp-ipv4 from 0.0.0.0 . Now when the instance is ready, grab its public ip and try to ping. We can see that it is unable to ping. Even we cannot ssh into it. This is because the subnet we created will be by default private subnet.
go to vpc and create a igw "my-igw". select it ,actions attach to vpc and select ur vpc and attach. But still we wont be able to access our server. 
select your subnet and click on route table tab. we can see there is no default route. we only have local route. For this lets create custom route table (we can also change in custom route table also). 
create route table public-rt --> choose our vpc --> create
click on subnet association tab, edit it and choose our subnet --> save association
Next click on routes tab and edit it and add default route i.e destination is 0.0.0.0/0 and target is the igw we created.
Now we will be able to ssh into our ec2 and ping our server.
-------------------------------
Nat Gateways:

Suppose we have server in private subnet and we need to give it internet connectivity to download and update security patches. we could attach IGW to this but the issue with this is now the server is in public subnet.which means it is open to entire world.
But incase if our servers are meant to be internal server, that only internal team should have access to, we dont want it to be accessed to internet.
We want our server to initiate connection to internet but there should not be any connection to the server from internet. For this we use NAT gateway. 
To use NAT gateway,we still need internet gateway. First we attach igw to vpc and then attach the NAT gateway to public subnet. (we can think NAT gateway as server running on public subnet). Next we need to setup routing so that devices in private subnet will have a default route that points to the Nat gateway. So that now instance will send packet to Nat gateway in public subnet and public subnet will send packet to internet. But we can never initiate a connection from internet to server as our server doesnt have public ip. 
Nat gateway is managed service. In pricing, it will be charged per hour and per GB of data processed.
Nat gateways are not region specific like internet gateways. 
When we deploy nat gateway,we deploy it to a subnet i.e it will be specific to az. if we want redundancy, we need to deploy it to multiple AZ for failover.
Nat gateway uses elastic ips.
Route table of private subnet should point to nat gateway.
A NAT gateway supports 5 Gbps of bandwidth and automatically scales upto 100 Gbps.
--------------------------------
Demo: Nat gateway

Create VPC,1 pvt subnet, create ec2 instance in that pvt subnet. Now our instance is created without public ip. before creating nat gateway, we need to have igw attached to our vpc as wkt we need to deploy nat gateway in public subnet and our subnet will become public after attaching igw. create my-igw and attach it to our vpc. next create public subnet in same vpc. Create 2 route tables, 1 for public subnet and 1 for pvt subnet. So that private subnet will have default route to nat gateway and public subnet will have default route to the igw. public-rt and private-rt . Edit public-rt and add default route to 0.0.0.0/0 to IGW. subnet association, select public subnet and save.
Edit private-rt and in subnet association, select private subnet and save.
click on nat gateway --> create --> my-nat-gateway --> choose your public subnet --> allocate elastic ip --> create.
go to route tables, select private-rt --> routes --> edit route and add 0.0.0.0/0 and choose ur nat gateway --> save
For resilience, you can create nat gateway in another az (for back up).
-----------------------------------
Private and Public subnets:

To determine whether a subnet must be private or public, we need to decide based on that should devices on internet be able to interact with our resources deployed on the subnet. If yes, then it should be public or else private. 
Eg. webserver application should be on the public subnet since users on internet needs to interact with the website, the webserver needs to be on a public subnet.
Suppose we have web application that needs to talk to database, in this case,webserver will be deployed on public subnet and database will be deployed on private subnet as it has sensitive information and only webserver should be able to talk to it.

Private subnet- use case:
Aws should act as an extension for our private data center, then we can deploy all our resources into a private subnet and use a vpn to connect our private datacenter to connect to aws resources.
-----------------------------------
DNS

By default all the private IP addresses assigned to EC2 instance will get a domain name.  
(eg. 10.0.100.10.ec2.internal)This is only on default private IP addresses. So for any resource that wants to talk to our ec2 instance can either send to ip address or its domain name.
By default only private ips get dns entry and public once cant. If we want public ip addresses also to get a domain name, we should enable "enableDnsHostnames" options when we create a vpc. 
enableDnsHostnames option determines whether the VPC supporting assigning public DNS hostnames to instances with public IP addresses.
enableDnsSupport determines whether the vpc supports DNS resolution through the Amazon provided DNS server.
AWS DNS server can be accessed on the second IP of the VPC CIDR block as well - 169.254.169.253
---------------------------------------
Demo: DNS 

Create a custom VPC. now select that vpc and actions --> edit vpc settings --> Under DNS settings, we can see two options "Enable DNS resolution" and "Enable DNS hostnames".
now check the box of "Enable DNS resolution" only and save.
Create a instance in this VPC and enable public ip address and create. Once the instance is created, in details section, we can see ip details. Where in we can see private IP DNS name (ip-10-0-1-144.ec2.internal) but no public IPv4 DNS and this is because we have not checked the box of enable DNS hostnames. And now if we check this box and save, then  
we can see the DNS for this public ip address(i.e ec2-35-173-226-213.compute-1.amazonaws.com) . Now using this DNS name, anyone can access our instance. It will resolve.
Next ssh into this instance to check the other option also. run cat /etc/resolve.conf , we can see the nameserver 10.0.0.2 . Now do --> nslookup google.com and we van see that using AWS dns server 10.0.0.2 , it was successfully able to resolve that. This worked because we have enabled the DNS resolution option in vpc. We can disable this and test it. nslookup wont work. This is because it is trying to send the dns request to that ip address 10.0.0.2 and AWS is now configured not to respond to it. 
------------------------------------
Elastic IP:

We get a public IP for a instance once we create it. but if we stop an start instance, it will change.
Elastic IP is a static IPv4 address. We can allocate it to our account and it becomes reserved for our account and then we can associate this eip with our instance. Now if instance is rebooted or moved to another account, the IP remains the same. 
We can also assign security group to this Elastic IP so that it will be fixed network configuration.
Suppose we want to do some maintanence on our instance,then we can move this elastic ip on another server which is running the same application.
Elastic IP pricing: EIP associated with running instance is zero cost. But if we associate another elastic ip with same instance, then we will be charged with Additional IP charged per hour. 
If we reserve an EIP and dont associate it with instance, it will incur small additional charge a well. 
EIp are specic to a region and cannot be moved to another region. EIP comes from Amazon's pool of ipv4 addresses and also from our custom ipv4 addresses.
----------------------------------
Demo: Elastic IP

create a instance in public subnet. note its public ip, now stop and start it, and we can see ip has changed. Select elastic ip in left side menu --> allocate elastic ip address --> choose region --> amazon pool of ipv4 --> Allocate.
Select this elastic Ip and actions --> associate elastic ip --> choose your instance --> in case if u have multiple private ips, u can choose that ip also --> associate 
ping this ip to test. stop the instance and again start it and now we can see it got the same elastic ip address.
Finally disassociate elastic ip from server and release the elastic ip
----------------------------------
Security groups and NACLs

They both act as firewalls. Firewalls monitor traffic and only allow traffic permitted by a set of predefined rules. They have inbound and outbound rules.
There are two types of firewalls, stateful and stateless.
stateless firewalls must be configured to allow both inbound & outbound traffic.
Stateful firewalls are intelligent enough to understand which request and response are part of the same connection. If a request is permitted,the response is automatically permitted as well in a stateful firewall. 

Network Access Control List (NACL):
NACLs filter traffic entering and leaving a subnet. NACLs do not filter traffic within a subnet. This means two servers within a subnet are allowed to talk to each other. 
NACLs are stateless firewalls,so rules must be set for both inbound and outbound traffic.

Security Groups:
Security Groups act as firewalls for individual resource (EC2,LB,RDS). 
Security groups are stateful,so only the request needs to be allowed. Response is automatically permitted.
Note:
* Security groups, when there are no rules,block everything. And when you add a security group rule,it allows a certain type of traffic.
All security group rules "allow" traffic; there is no "deny" option for security groups.
* NACL rules can either allow or deny traffic. They have Rule number (70,80,90), smaller the number and earlier it will get processed. It also has Allow/Deny option.

Multiple Security Groups:
* You can assign multiple security groups to a single resource.
* The rules for bot sg will get merged.

* By default,sg contain outbound rules that allow all outbound traffic (you can delete this rule)
* Every subnet within a VPC must be associated with a network with a network ACL
* You can associate a network ACL with multiple subnets;however,a subnet can only be associated with only one network ACL at a time.
-------------------------------------
Demo: Security Groups

We can create one sg and the same sg can be used for other resources like lb also. when we create instance, by default it gives access to only one rule is ssh --> port 22 --> anywhere. once instance is created,delete the sg.
Create a new security group "webserver-sg" ,choose vpc ,inbound rule allow ssh traffic.
Now to attach this sg to the instance, select ur instance ,actions --> security --> change sg --> remove the old sg --> choose "webserver-sg" and click add --> save.

ssh into this instance and install a webserver 
--> sudo yum install nginx
--> sudo systemctl start nginx

--> curl localhost (o/p: welcome to nginx)
Now if we grab its public ip and access it in browser, we cant and this is because we need to allow inbound port 80(HTTP) and 443(HTTPS) from anywhere. 
Even if we delete outbound rule, sg is smart enough to allow traffic outside as it is stateful firewall. but from inside our server, if we do ping 8.8.8.8 ,it wont work. so we need to add outbound rule.
We can attach the same sg to multiple servers.
if we want the traffic to come from particular instance to our server, we need to copy that instance pvt ip and paste it in our security group and say allow custom tcp from this ip (OR) we can also add the security group from which we want to allow traffic to this security group. this means "the traffic from any resources which has this sg associated with it is allowed" so by this we dont keep track of IPs of individual resources. 
------------------------------------
Demo: NACLs

Go to your ec2 instances and allow all traffic in sg for testing purpose. Identify the subnet for your intances in the networking tab. Go to VPC and choose Network ACLs in the left menu.select that subnet and we can see in the inbound rules, all traffic rule number 100 is Allow. And another rule below it is All traffic Deny. Traffic doesnt hit deny as it is not having rule number. 
Now edit inbound rules of this subnet and modify rule 100 to allow only ssh from anywhere. by this everything else traffic will hit deny. This is same for another ec2 instance also as they both are in same subnet. but now if we try to access our site in browser, we cant. as we are blocked.
Next go back to NACL and edit that subnet & allow http (rule no 110) and https (rule no 120) and save. Now we can access our site.
Next if we try to install nginx in the server-2 , we cant. This is because of NACl it cant reach internet. Even if we have all traffic allowed in the outbound rules,it cant reach the internet. This is bacause NACLs are Stateless.We need to explicitly mention the outbound rules. Now edit inbound rules again and add rule number 130 and allow all traffic. Now our server will be able to access internet. We need to allow rule on both sides for Nacls.  Now ur site "welcome to nginx" will load fine. now we can remove the all traffic from the inbound rules of the Nacls subnet. 
We can make use of Nacls when we want to filter some traffic .eg. in the same subnet inbound rule, edit it and create rule 90 and deny ssh from 1.0.0.0/24 . so now other then the mentioned ip, all others can do ssh into servers.
This is a benifit of nacls over sg as they can be configured for both allow and deny whereas security group for only allow.   
-----------------------------------
Load Balancers:

Consider we have our app in ec2 intance and user is accessing it. suppose if our instance goes down, then application goes doen and it cant be accessed. to avoid we deploy same application in 3 other servers in different AZ as well. But each of these instances will hae different IPs. What ip address does the user send request to? Instead of letting our user know all ip addresses, we can make use of Load balancers. So now only IP address the user need to know is the ip of load balancer.

Load Balancers in AWS - Types
1. Classic load balancer
2. Application load balancer
3. Network load balancer

Classic load balancer is old generations and has lot of limits. One of the limitations is that we can use only one SSL certificate per classic load balancer. If we have 2 different applications, then we can use only one SSL recommended. hence this lb is not recommended for new application setup.

Application lb works for web based applications and they support HTTP/HTTPS/Web Sockets. And if your application is using some other protocol, you will not be able to use App lb.
They function at application layer (layer 7).
They can forward requests based off of: 
* URL Path conditions,
* Host domain,
* HTTP fields - header,method, query, and IP
* Supports HTTP redirects and customHTTP response
They Perform application-specific health checks

When the client sends request to ALB using any protocol say SSL/TLS, Http/Https are always terminated on ALB.

             SSL/TLS                               HTTP
client  --------------------->   ALB   ---------------------------> EC2 instance
                        SSL certs resides on ALB             (unencrypted)
                       (Encrypted only upto here)

If you still want the traffic to be encrypted between load balancer and EC2 instance,We can make it by adding SSL cert to our EC2 instance as well.We should manage it ourself.

3. Network loadbalancer(NLB):
* Load balance traffic based on TCP/UDP (layer 4)  (It doesnt understands HTTP or HTTPS)
* Meant for applications that don't use HTTP/HTTPS
* Faster than application load balancers
* Health checks are only basic ICMP/TCP connections
* NLB forwards TCP connections to instances. (nothing is terminated here in between)

Since the session is between client and EC2 instances, the traffic is encrypted end to end
            TCP/UDP                             TCP/UDP
client ----------------------->   NLB   ---------------------------> Instance
                       (NLB forward based on UDP/TCP)               (certificate)

They use TLS/SSL for TCP and DTLS for UDP

Network Load Balancer (NLB) can forward HTTP and HTTPS traffic also because these protocols are built on TCP, NLB does not offer features specific to HTTP/HTTPS traffic management, such as SSL/TLS termination or URL-based routing. For applications requiring such features, an Application Load Balancer (ALB) would be a more appropriate choice.

Elastic load balancers work description:
consider we have a vpc and two AZ inside it and our resources are inside it. When we configure ELB here, it will ask for AZ details and we can define this by specifying the subnets.(there are physical resources that get deployed when we are using the load balancer). So we create a subnet in each of the AZ that we want to load balance traffic to and when we select those subnets, AWS will deploy 'LB node' on each of these subnets. And they are responsible for load balancing traffic. Once they are deployed onto the subnets we specify,we can now load balance traffic to any other subnets or even same subnet within that AZ.
When the user or the client wants to send traffic to one of the instance, A DNS record is created for the ELB. And that DNS record is equally forwarded among all load balancer nodes. And taht LB nodes will direct load to respective EC2 instances. 

Cross-Zone Load Balancing:
Consider we have a vpc and two AZ and a ELB and LB node in each AZ. When user sends request to ELB, it will be equally distributed among nodes. And this node will then distribute load among ec2 instance equally (if two instances are there, then 25 % each).
And all these are in same AZ and the traffic cant go accross AZ. And hence Cross-Zone Load Balancing feature was created. It allows LB nodes to load balance traffic to ec2 instances and resources in different AZs.

Load Balancers - Deployment Modes

Public load balancers:
* Deployed on public subnets
* Access by users across the public internet

Private Load Balancers
* Deployed on private subnets
* Access by users within the Organization's AWS Network

Consider you have a Api layer with two instances deployed in public subnet and it has a load balancer attached to it. Here client can send request to these instances through this load balancer as they are in public subnet. Now cosider we have one more layer of database with two instances in private subnet. It is also having a loadbalancer but it is a private load balancer. Now client wont be able to send traffic to this private load balancer from outside the vpc.In the same vpc, the instances or loadbalancers in pubic subnets can forward requests to resources in private subnets. 

In configuring LB, we have two things. Listeners and Target Groups.
Listener is the process that checks for connection requests using the protocol and port that we configure.
there are variety of listeners. We can listen for requests that have specific http methods. If we are using ALB, we can listen for specific hostnames. Any requests that are destined for app1.com we can configure a listener and make it forward it to a target group. 
Target groups route requests to one or more registered targets such as ec2 instances using the protocol and port number that we specify. Even ECS and lambda functions can be configured as target groups.
we may have requests coming for app2.com/auth ,we can forward it to different target groups.which may be ECS. And we may have requests coming for app2.com/cart ,we can forward it to different target groups.which may be lambda functions.
We can also configure health checks on particular target groups. Health checks are performed on all targets regestired to a target group that is specified to a listener in load balancer. Suppose if any instance fails health check, we can stopt routing traffic to that instance.
----------------------------------------
Demo : Load balancer

Create 2 ec2 instances webserver-1 and webserver-2 both created in different AZ us-east-1a and us-east-1b and both are public servers. This is a good design as we have redundancy for AZ failure. And we have two public subnets for them. 
Now lets configure load balancer to balance load across these two instances. Create two extra public subnets for loadbalancer. 
Now come to EC2 page and choose loadbalancer in the left menu. 
create load balancer --> application load balancer --> Name: web-lb --> Internet-facing --> IPv4 --> choose vpc: vpcdemo --> Choose 2 AZ and atleast 1 subnet per AZ : lb-us-east-1a and lb-us-east-1b subnets --> security group --> Listener and routing : HTTP port 80 (but we can change our port to any port number here) --> create a target group --> instances --> name: tg-web --> port HTTP 80 (as our app is listening on default port 80) --> choose vpc --> HTTP1 protocol version --> health checks : HTTP path: / -->  Next --> register targets : select the 2 instances -> port:80 -> Include as pending below -> create target group
Tg is created Now refresh in the app lb creatin page and choose your tg (we can add more listeners here like https) --> create lb.
Once the lb is created, select it and in details we can see DNS name of this lb ,copy it and paste it in browser and we can see our page . we can refresh page to see the page content server1 and server2 .
Now instead of keeping our servers in public subnet, we can keep them in private subnet so that traffic of users will always go through load balancer.only lb should be on public subnet.This gives extra security.
-------------------------------------
VPN (Virtual Private Network):

Assume that we have a vpc which contains a private subnet with certain number of resources deployed onto it. suppose we have a On-premise network and devices in our datcenter needs to talk to the resources in pvt subnet, and we need to be able to safely and securly establish connection between them, then we use VPN here. We deploy the vpn in our vpc which allows the connection between on prem datacenter to the resources in pvt subnet and it will be end to end encrypted.
                                      IPSec
               VPN Gateway -----------------------------> customer gateway
                                  VPN Connection

VPN architecture in AWS:
Consider we have a VPC with CIDR block of 10.0.0.0/16 and we will have pvt subnet and have some resources within those subnet. We need to deploy the VPN gateway and it will terminate VPN on the AWS side. now consider we have on-prem network with CIDR block 192.168.0.0/16 and we need to deploy customer gateway in here. Customer gateway terminates VPN on the Customer side (It can be attachable to one VPC). Both customer gateway and VPN gateway will get public ip address (eg. 1.1.1.1 and 2.2.2.2 respectively). Now we can establish IP sec tunnel between them over the internet.

VPN Routing:
We have 2 types, static method and dynamic method. In static method, in which we manually define a route in the AWS routing table where we specify the IP of the on prem and for this VPN will be pointed and connection will be established between 2 sides.
In dynamic method, We can configure dynamic protocol to exchange routes dynamically using BGP. So now VPN gateway will know how to get to that specific on prem network.

VPN pricing:
* Charged for each available VPN hour.
* Charged for data transfer out from Amazon EC2 to internet.

Note: In AWS, we are charged generally for outbound traffic and not the inbound traffic.

VPN Gateway limits:
* Maximum bandwidth per VPN tunnel of 1.25 Gbps
* Maximum packets per second (140000)
* Maximum transmission unit (MTU) (1466)
--------------------------------------
Direct Connect: (for compliance reason)

It is a physical connection into AWS.An altrnate to VPN. In VPN, The data goes over the internet and internet can be unstable and un reliable hence Direct connect was introduced.
                           Direct Connect
corporate Datacenter --------------------------> AWS resources 

Direct connect Architecture:
consider we have On-Premise Network which has a customer Gateway . now in between this on prem and Aws resources, direct connect location which may be not owned by AWS and they may rent some space here to have routers in there. In this location, We have both customer Router and AWS router. Between them, cross connect is established which is the connection between a port on AWS router and customer router. and now vpn gateway is set up in the AWS for our vpc. and finally aws router will connect with VPN gateway and connection is established. We can also reach public services directly from AWS router and no need for VPN. 
Direct connect will be faster and relible and low latency.

Direct Connect pricing:
* charged for Port Hours
* charged for Outbound data transfer.
---------------------------------------
VPC peering:

* By default resources in two different vpc cannot talk to each other since VPC acts as a network boundary. If we want to establish connection between them, we need to use VPC peering.
VPC peering is the network connection between two vpc and it handles routing traffic between vpcs.
With VPC peering, we can setup connection between VPCs in same region and vpcs in different region and also VPCs in different AWS accounts.  

VPC peering pricing:
* NO cost for VPC peering connection creation. 
* Data transfered within an AZ via VPC peering is free.
* Data transfer across VPC Peering between AZ incurs charges. 

How does it work?
Consider we have VPC 1 with CIDR 10.1.0.0/16 and VPC 2 with CIDR 10.2.0.0/16 . One VPC will send request to another for initiating peering. so the owner of VPC 1 will send peering request to owner of VPC 2. Once the request is sent, the owner-2 will except the peering request. Now peering is established between these VPCs. Next we need to handle the routing.this is to tell our VPC one about how the get to the 10.2.0.0/16 CIDR block. For this we need to create the route table where we define the CIDR of VPC 2 and target where we specify unique identifier of our Peering. So that any traffic from VPC 1 to VPC 2 will match this route and it'll notice send it across the peering.
We need to do the same thing in the VPC 2 also by adding the CIDR of VPC 1 in routes.Now the instances in two different vpcs can communicate freely. 

In case if we have 3 VPCs,and we have peering between VPC 1 to VPC 2 and VPC 2 to VPC 3.
Now VPC 1 cannot talk to VPC 3 using VPC 2 a transitive. If we want VPC 1 and VPC 3 to talk to each other then we need to setup their own VPC peering as well.
---------------------------------------
Demo: VPC peering

Create two VPCs VPC-A with CIDR 10.1.0.0/16 and VPC-B with CIDR 10.2.0.0/16. Create Server-1 in VPC-A and Server-2 in VPC-B. Login to Server-1 and try to ping the pvt ip of server-2.Ping is not successful. 
Lets setup the VPC peering. Go to Peering connections in left menu in VPC --> create peering connection --> name: vpca to vpcb --> Requester: VPC-A --> Select another account for VPC to peer with (My account and this region) -->  Accepter VPC ID : VPC-B --> create. 
Now in the peering connections, we have a "pending acceptance" status. Select it and actions --> Accept request. Now peering is successful.  But still we will be unable to ping the ip of server-2. We need to set up route between them. 
Go to Route tables,select route table of VPC-A ,in the routes tab, we can see local route and default routes(igw) only.   
Edit routes and in destination add 10.2.0.0/16 and in target choose peering connection: vpca to vpcb --> save
Now VPC-A know about CIDR of VPC-B but still VPC-B doesnt know about VPC-A.
In VPC-B add routes of 10.1.0.0/16 and choose vpc peering in target --> save
Now try to do ping and ping is successful.Its all going through AWS infrastructure only and not the internet. 
--------------------------------------
Transit Gateway:

WKT VPCs cant talk to eachother by default. To allow this,we need to setup vpc peering connection between them. As the number of VPC increases, it will become difficult to manage them by peering them each other. And if we have 4 VPC and we need to connect to them from on prem, then we need to setup vpn connection from customer gateway to all 4 VPC seperately. By this number of VPN will also increases. To address this issue, a service called Transit Gateway was created.
The idea behinf Transit gateway is to avoid creating full mesh of VPCs and having to maintain all of them. 
* Transit gateway acts as transitive routing device to route traffic between VPCs.
* We must specify 1 subnet for each AZ to be used bt the transit gateway to route traffic. 
Now consider we have 4 VPCs and we deploy a transit gateway. and these 4 VPC only need to peer or connect with transit gateway and no ned to peer with one another. And the transit gateway acts as a router that routes traffic between VPCs.
With Transit Gateway, we no need 4 VPN connections, instead just connect the on prem customer gateway with Transit gateway. In same way we can also utilize direct connect.
one Transit gateway can peer with other Transit gateway within same region or also between one region to another region and also transit gateway in different AWS account.
---------------------------------
Private link:
Consider we have EC2 instance deployed in pvt subnet. And if this instance need to access S3 bucket, then we need to provide access to internet by creating NAT gateway or internet gatway. By doing this our EC2 instance will be exposed to internet which we dont want. This is where Private links are used.
Private links give your VPCs direct access to the public services like s3,lambda,cloudwatch etc so the traffic doesnt need to route through the internet.and our instance is more secure now. 
Private links also allow the resources in one VPC to connect with services in another VPC using pvt ip addresses. Suppose if a third party company was providing some service and we need access to it,then we can create private link to the VPC that service runs into and give direct access as if it was hosted directly in our own VPC
------------------------------------
CloudFront:

In the concept of global content delivery and edge locations, consider we have a webserver in N.America and if a user in N.America can access the applications without any latency but for the user in India, there will be high latency due to long distance.
To solve this issue,AWS provides Edge locations. AWS has smaller edge locations scattered accross all globe. our web application files will be cached in these edge locations. Now the user from remote locations can send request to these edge locations instead of long distance webservers.This reduces latency. Cloudfront is basically a CDN. 

Cloudfront is a web service that speeds up distribution of static as well as dynamic contents such as HTML,CSS,JS,media files like images,videos,song etc. so that users get access to them very fast. 
When the user sends requests to cloudfront, their requests will be routed to Edge location first and they provide lowest latency or time delay so that content is delivered with best possible performance.

Cloud front architecture:
There is 'origin'. Origin is the source location for content that will be cached by cloudfront. Eg. images and files stored in S3 bucket is a origin. And cloudfront will take these files from origins and cache them in the edge locations.
Now suppose we are configuring cloudfront, after origin,we need to configure distribution. Distribution is a configuration unit/block in cloudFront. In Distribution we tell cloudfront where we can find origin/source files, and cloudfront will create a dynamic domain name for us (eg. http://xyz.cloudfront.net). We can now access cached images at edge locations by using this domain. 
If a user sends request to Edge location and if edge location doesnt have that content,then edge location will forward request to origin and get the content and display it to user.And now that content will be cached at edge location for fast retrieval.  
cloudfront TTL:
* Cached content at edge location remains for a set time known as time to live (TTL). 
* TTL value decides content validity before an edge location requests the origin.
* Default TTL is 24 hours.
* Can be configured to have objects expire at a specific time. 

Cache invalidation allows you to invalidate content cached at edge locations.
suppose we have the version 1 of the file in origin and it is cached at edge location and now we have updated that file to version2 in the origin but the edge location still have version1 of that file and it will stay there till 24 hrs and the users will see the cached content only. To address this issue, we do cache invalidation which will remove all cacheed content in edge locations. and now if the user sends the request to the edge location, those files wont be there and it will send request back to origin and gets the new version2 file. Cache invalidation is done before the TTL expires.

Cache Invalidations:
* Invalidations are performed on a distribution. (we invalidate individual distributions)
* You can invalidate all objects in a distribution,a specific folder,or a specific object. 
* /* - Entire disribution invalidate.
* /file.txt - individual file.
* /images/* - All objects in images directory

CloudFront - Basic integrations with Other Services.
SSL/TLS is enabled by default and default domain name is https://xyz.cloudfront.net and AWS will provide default SSL certificate *.cloudfront.net and we can also setup our custom domain name: https://kodekloud.com where we can utilie AWS ACM and we can create certificate for our cloudFront distributions by using custom certificate.
* Cloudfront will Automatically publishes operational metrics for distributions. 
* We can enable extra metrics for additional cost as well. 

Use cases:
We can cache Static websites,video on demand and also for streaming
--------------------------------------
Demo: cloud front

In AWS , go to S3 and create bucket with publically accessible. Upload a image to bucket. In the bucket policy, add the policy for s3:GetObject and save it. now we can access the file using object URL. Go to cloudfront and for origin domain choose your S3 bucket url --> In origin path we can choose the particular path if we have subfolders in our bucket (eg. /images) or else leave option empty --> name --> origin access : "Public" (choosed) (if users dont want to use cloudfront distribution,they can access s3 bucket directly for which bucket must be public) (or) "origin access control settings" (bucket can restrict access only to cloudfront) --> Default cache behaviour : Compress objects automatically: yes --> Protocol HTTP and HTTPS (or) Https only --> Allowed methods: Get,Head,put,post --> Web Application Firewall (WAF): Do not enable security protection --> use all edge locations --> if u have certificate,u can choose here --> create distribution
Now our distribution is created. select it and we can see the Domain name. And now if we access this url, it wont load because we need to mention the name of the image as well along with this domain as a context path.This  domain name is accessing the edge location and not the S3 bucket. Now if we delete this image from s3 bucket,and upload another image with same name. But if we access the cloudfront domain,we still se the old image. This is because image is cached and it exists untill the ttl expires.
Go to cloudfront distribution and in the "Behaviours" tab, and there will be default which is 86400 sec(1 day). so we need to do invalidation now.
go to the "invalidations" tab --> create invalidation --> Add object path : /* --> create invalidation
Now if we access the distribution domain, we can see the new image. 
------------------------------------
Lambda@Edge

Cloudfront functions and Lambda@edge:
Using this feature, we can write light weight functions that can run at these edge locations and these functions can perform variety of operations like manipulating the requests and responses that flow through cloud front, also they can perform basic authentatication,authorization as well as generate responses at the edge so that we dont need a physical server now so that all the logic happens at edge location itself.

When do functions Run?
Cloud front functions:
* Cloudfront funcs will run when cloudfront receives a request from a viewer (viewer request).
* Before Cloud front returns the response to the viewer (viewer respose) 

Lambda@edge functions:
* When cloudfront receives a request from a viewer (viewer request)
* Before cloudfront forwards a request to the origin (origin request)
* When cloudfront receives a response from the origin (origin reponse)
* Before cloudfront returns the response to the viewer (viewer response)

Cloudfront functions use cases: (for running small functions)
1. Cache key normalization (transform http request attributes to create optimal cache key which improves caching ratio)
2. Header manipulation (we can insert,modify,delete http headers)
3. Url redirects or rewrites
4. Request authorization (validate Json web tokens by inspecting authorization headers)

Lambda@ege use cases:
* Long-running functions
* configurable CPU and memory functions
* Dependencies on third-party libraries
* Network-dependent functions 
* File system or HTTP Request Access functions
------------------------------------------
Global Accelerator: (for optimizing paths)
Consider our application deployed in N.America and user access it through remote locations, then the request will go through the internet and they have inefficient path, they are not reliable as well and latency will also be more. Global accelerator will help address this issue so that we can improve the experience of end users.
When we enable Global accelerator, we have access to all their edge location. These edge locations are different compared to cloudfront edge locations. Now when a user sends the request so they can talk to our application,it will reach the closest global accelerator edge location. By this user will no need to go through regular internet and instead they go through AWS backbone network. So now it will be much faster connection and secure.
AWS backbone is lot faster compared to other regular internet providers. 
--------------------------------------------
Route53:

* An AWS managed DNS service 
* Acts as a domain registrar - purchase domains similar to sites like Godaddy and Namescheap (eg. www.kodekloud.com)
* A global service - not specific to a region

Hosted zones: collection of DNS records for specic domains.
Our domain name will be created using hosted zone. We define all of our records and rules in Hosted zone. When we create hosted zones for specific domains, AWS will reserve/allocate 4 nameservers to hosted zones.
-------------------------------------
Demo: Route53

Go to rout53 --> domains --> registered domains --> enter your domain and checkits availability : kokeklouddemo123.com -> select it --> proceed to checkout --> check box of auto-renew --> next --> provide you contact details --> Verify (we also incur small charge for hosted zone) --> Submit
Wait for some hours and now we can see our domain name in the list.And we can see the 4 nameservers allocated to this. Go to hostedzone and we can see one hosted zone. And in that we can see dns records configured automatically. We can create more DNS records by clicking on create record --> A record (to point to ip of our webserver) --> Recordname: www or leave it empty (subdomain) and paste the ip in the value --> create record
view status and wait till u see INSYNC.
Now go to browser and enter 'kodeklouddemo123.com' and we can see our web page. 
---------------------------------------
Route 53 Application Recovery controller
It is a service that contineously monitors applications ability to recover from failure. And to control app recovery accross multiple AWS regions and AZ. 

Consider we have application deployed in one region and a stand by app deployed in another region. we can use Application Recovery Controller to monitor backup environment. Make sure its up and running,scaled up and configured properly so that we know that when the time comes that do we need to failover to it, it will be up and running. Based on our previous configuration, we can have Application Recovery Controller to monitor our primary deployment. If any issue in our primary app, then we can failover to backup site. We can divert all trffic to backup site untill we figure out the issue in primary site. 
We can also use ARC to manually diversion of traffic. 

Application Recovery Controller Architecture:
Consider we have our active application in us-east-1 region with lb and ec2 (this full this is cell-1) using a global dynamo db (resource set) and we have a failover setup in us-west-1 region with same setup (cell-2). We have route53 on top of these which sends traffic to active region and if any issue, we can modify in route53 to send traffic to standby region.
* cell groups all resources required for an application to operate independently. 
* Recovery group is a collection of cells that represent an application that i want to check for failure readiness. It monitors both active and standby site.
* Resource set is a set of AWS resources that can span multiple cells. 
We can setup Application Recovery Controller to do readiness checks to monitor the readiness of the standby deployment.
* Routing Control allows you to manually failover entire application stack to standby site. And finally Route53 will do the failover.
-----------------------------------------------------------------------------------------------------------------------------------------------------------
**** Services: Storage

Elastic Block storage (EBS):
Block storage breaks up data into blocks and then store these blocks as seperate pieces each with a unique identifier. And these blocks can then we stored across number of physical devices. A collection of blocks can be presented to you OS as a volume.Then we can create File system on top of it. We can also present it as a hard drive which allows us to install OS on it to boot up.
* So Block storage is both mountable and bootable. (IMP)  
AWS Elastic block storage (EBS) is the same.
We can attach EBS to an instance,detach it and attach it to another instance. And all data remains intact.
Normally EBS volumes can be attached to a single ec2 instance whereas certain volumes can allow multi attach where multiple instances can attach to same volumes. Where our application must be intelligent enough to not have multiple ec2 instances write to the same data at same time and this may lead to curropt the data. 
Eg. DB clusters will be intelligent enough to have master node responsible for writing to data and worker nodes just read from it. 

EBS volumes must be provisioned in an AZ.Redundancy is built in within the AZ. It can handle physical device going down but data is lost if entire AZ goes down. 
To attach ebs volume to ec2 instance, both must be in same AZ. But incase if we want the data in the volume of one AZ to be available in another AZ,then there is a feature called "snapshot". We create a snapshot of the volume in AZ1 and from this snapshot we can create new volumes in another AZ2.And then we can attach this newly created volume to EC2 instance.Snapshots can be stored in S3. 
Next to get the data between regions, again we can take snapshot of ebs volume in region1 and keep it in s3 bucket and then we can copy the snapshot to another s3 bucket in another region and finally create volume out of this snapshot.

EBS volume types: 
1. General purpose SSD [gp2 & gp3] 
2. Provisioned IOPS SSD volumes
3. Throughput Optimized HDD volumes.
4. cold HDD volumes
5. Magnetic

General purpose SSD [gp2 & gp3] :  {max 16TiB vol size, 16,000 iops per volume, 1000 MiB/s max throughput per volume(gp3)}
These are volumes backed by SSD (Solid state Drives). They give better pricing and performance. we can use them for wide variety of workloads like virtual desktops,medium size,single instance dbs,latency sensitive interactive applications, as well as dev and test environments. We can startoff with SSD. They dont support multi attach.
There are 2 types gp3 and gp2 volumes.
* gp3 volumes are latest generation SSD volumes and lowest cost volumes offered by aws.  
this Volume type provides right balance of price and performance for most apps.It aso helps scale volumes performance indepedently of volume size.gp3 offers 20% lower price compared gp2 volumes. 
* gp2 volumes are default volume types for ec2 instances. They offer cost effective storage ideal for broad range of transactional workloads.The performance scales with volume size.

Provisioned IOPS SSD volumes (io1 and io2):
They are backed by SSD (Solid state Drives)
These are the heighest performance EBS storage volumes. They are designed for critical,IOPS intensive,Throughput-Intensive workloads.
The different versions available are,
1. Provisioned IOPS SSD (io2) volumes   {max 16TiB vol size, 64,000 iops, 1000 MiB/s max throughput per volume}
2. Provisioned IOPS SSD (io2) Block Express volumes      {max 64TiB volsize, 256000 iops , 4000 MiB/s max }
3. Provisioned IOPS SSD (io1) volumes.

io2 has more durability compared to io1 volumes (99.999% durability). They support multi attach. used for high intensity workloads like db workloads needs more performance.

Throughput-Optimized HDD and Cold HDD volumes.
They are traditional HDD based volumes and they can be slower. Thre are 2 types.
1. Throughput-optimized HDD : Low cost HDD designed for frequently accessed throughput intensive workloads
2. Cold HDD: It is the lowest cost HDD designed for less frequently accessed. (cheapest)

Throughput-optimized HDD {500 iops/vol, 500 mib/s max throughput per vol}
Cold HDD {250 iops/vol, 250 mib/s max throughput per vol}
max vol size for both 16 TiB

Magnetic volumes:
They are previous genertions of volumes backed by magnetic drives. Suaitable for dataset,infrequent data access and performance is not of primary importance.Max iops of 200 and vol size upto 1 TiB

EBS pricing:
We are charged per GB per Month. Cost also depends on the volume type selected. Faster IOPS more cost. Snapshots are also charged per GB per month. This is for full snapshot and not for incremental snapshot ,we are paying full cost. 

The thing that seperates block storage is that we can both Boot and mount block storage unlike file system storage and object storage.(IMP for exam)
------------------------------------------
Demo: EBS Part1

Create 3 servers in us-east-1 region , in which server-1 and server-2 in us-east-1a AZ & server-3 in us-east-1b AZ and server-4 in us-west-2 region
Go to volumes and create volume --> gp2 --> 10 GB --> AZ : us-east-1a --> enable encryption --> create
Give a name: demo-volume ,it will be in available state. select it and actions --> attach volume --> choose instance :server-1 --> devise name : /dev/sdf (but it can also be /dev/xvdf thru /dev/xvdp) --> attach
Login to server1 and run lsblk to list our all the volumes attached to this. We can see output xvda and xvdf. xvda is the root / volume that we got when we created our instance. the sdf got renamed to xvdf as per the note while attaching volume. First we need to check if there is any file system in the attached volume 
--> sudo file -s /dev/xvdf       (o/p: /dev/xvdf: data  --> indicates that there isn't any file system currently)
Now lets create a file system on this volume
--> sudo mkfs -t xfs /dev/xvdf
Now the file system is created. Now re run the previous command to see the fs.
Now lets mount this fs in a directory 
--> cd /
--> sudo mkdir ebsdemo
--> sudo mount /dev/xvdf ebsdemo 
--> df -kh       (to see the mount)
This is a temporary procedure to mount and if we reboot the volume, it will no longer be mounted. In order to make this permanent,we need to configure fstab for which first we need to get the unique identifier for our ebs volume.
--> sudo blkid  (o/p: /dev/xvdf: UUID="045ddc53-453-986c") and copy it
--> sudo vi /etc/fstab
We can notice how other mounts are setup and add a new line 
UUID=045ddc53-453-986c  /ebsdemo  xfs  defaults,nofail  (add this and save file)
--> sudo mount -a  (this will automatically mount whatever listed in the fstab file)
--> cd /ebsdemo/
--> sudo echo "this is test comment on server1" > demo.txt

Now lets detach this volume from this instance and attach it to another instance
--> cd /
--> sudo umount /ebsdemo 
--> df -kh  (verify it)
go to volumes select it and actions --> detach volume. again select it and attach it to server-2 (in same AZ). login to server-2 now and verify if it is attached.
--> lsblk   (/dev/xvdf)
--> sudo file -s /dev/xvdf  (we can see the file system because we created it in another machine)
--> cd /
--> mkdir ebsdemo
--> sudo mount /dev/xvdf /ebsdemo
--> cd ebsdemo
--> ls   (o/p: demo.txt)

Like this we can perform these steps if we are performing maintanance or migrating our data between instances.
------------------------------------
Demo EBS : Part-2

Now lets move our ebs volume to server-3 which is in different AZ. 
Unmount your ebs volume first
--> sudo umount /ebsdemo
--> df -kh
Detach it . now select volume again and we need to create snapshot --> name: my-snapshot --> create snapshot
once it is availabe, select that snapshot --> actions --> create volume from snapshot --> choose AZ: us-east-1b --> encrypt --> tag --> create.
our volume is now created in us-east-1b and now we can select it and attach it to server-3 on /dev/sdf --> attach
login to server-3 and lsblk
--> sudo file -s /dev/xvdf  (o/p: xfs filesystem is present)
--> sudo mkdir /ebsdemo
--> sudo mount /dev/xvdf /ebsdemo
--> cd /ebsdemo/
--> cat demo.txt

Next inorder to copy the volume to server-4 in different region, we cannot follow the same steps as before. For this we need to copy it to s3 bucket or to desired region.
Select the snapshot --> actions : copy snapshot --> description: copy of my snapshot -->Destination region : us-west-2 --> copy snapshot
Now we can go to us-west-2 and see the copy of our snapshot.select it and create a volume in this region in same AZ as our instance and finally attach it to server-4. Follow the remaining steps from above
----------------------------------
Instance store

Instance store provides temporary block level storage for our ec2 instances. Depending on ec2 instance type, AWS may provide instance store which is temporary block level storage.
Instance store are located physically on disks attached to the host computer. Unlike EBS volumes, where ebs data is stored on seperate m/c and we connect to it remotely, Instane store is physically on host computer thst your EC2 instance run.
Instance store is only meant for temporary storage or information that changes frequently like tempory content or scratch data. 

Instance storage Architecture:
consider we have 2 ec2 instances running on our host machine. as the instance store are physical disks that are on the host machine, so when our EC2 instance go down ,and come back online it will still have access to instance store. Even if it is temporary,if the ec2 instance is on same host, then it will always have access to same instance store. 
Problem arises when te EC2 instance goes down and come back on another host-2. So now it wont be able to access that instnce store as it recide on completely seperate host. But instead The new ec2 on another host will have access to instance store in that particular host. But all of the data it had on instance store 1 is now gone. It will be starting with a blang state on instance store 3. Thats why it is called temporary storage.
---------------------------------------
Demo: Instance store Volumes

go to EC2 --> launch instance --> not all ec2 instance type support instance store so choose m5dn.large --> choose key pair --> under volume section, we get instance store volume,we get volume2 with size 75GB and device name /dev/nvme0n1 --> enable public ip --> launch instance
Login to the instance and do lsblk to see the attached volumes (o/p: we can see / and nvme1n1)
--> sudo file -s /dev/nvme1n1
--> sudo mkfs -t xfs /dev/nvme1n1
--> sudo mkdir /instance-demo
--> sudo mount /dev/nvme1n1 /instance-demo
--> df -kh
--> cd /instance-demo
create a file with text
Now ec2 instance moving to other physical host doesnt mean that after reeboot the instance will move to otherhost,it will still be in same host. 
Test it by rebooting but you can still see the same public ip for the instance. Thats how we know if our ec2 has moved to different physical host or not. 
Again login to instance and run mount command again and we can see the file we created.

This time stop the instance and later start it and the public ip will change. Now our instance has moved to different host. Now it is going to get the brand new instance store volume as old one was in the other physical host. Now login to this instance
--> lsblk  (o/p: nvme1n1 is still attached)
--> sudo file -s /dev/nvme1n1  (o/p: /dev/nvme1n1: data)
This means all the data is gone as there is no file system

Remember that instance store volumes are not ment for data that persists. It is only for temporary data.
--------------------------------------------
EFS (Elastic File System)

There are 2 types of file system provided by AWS
1. AWS EFS
2. AWS FSx

EFS uses network file system v4 protocol NFSv4. Any application using this protocol can work with AWS EFS. With EFS we create a file system and EC2 instances and other computer services can remotely connect to that file system and mount them.
EFS only works with only linux. With EFS file system, data can be shared across multiple EC2 instances at the same time.
To create EFS , we deploy it onto VPC (EFS is VPC specific). Then inside VPC,it is made available to AZ using mount target and once they are deployed into subnets they get IP address. eg. 10.0.1.10  
EC2 instances can then connect to the file system by specifying the IP address of the mount target. For high availability, we can have mount targets in multiple az (eg. 10.0.2.10 in az2).  
EFS offers following storage classes. 
1. Standard storage classes like EFS Standard and EFS standard-Infrequent Access (Standard-IA). They have multi-AZ resilience and the highest levels of durability and availability.  
2. One zone storage classes like EFS One-zone and EFS One Zone-Infrequent Access (EFS One Zone-IA). The choice of additional savings by choosing to save your data in a single Availability Zone.

EFS also provides different performance modes that handle different throughput,Iops,low latency needed for a broad range of workloads.
1. General Purpose Performance Mode: suitable for latancy-senstive applications like web-serving environments,content-management systems,Home directories,General file serving.
2. Elastic throughput Mode: Automatically Scale Throughput performance up or down to meet the needs of your workload activity.
3. Max I/O Performance Mode: Higher levels of aggregate throughput and operations per second. They have higher latency for file system operations. 
4. Provisioned throughput mode: Level of throughput the file system can drive independent of the file system's size or burst credit balance. 
5. Bursting throughput mode: Scales with the amount of storage in your file system and supports bursting to higher levels for up to 12 hours per day. 

Installing amazon-efs-utils
--> sudo yum install amazon-efs-utils -y
--> sudo mount.efs efs:id /directory
The efs:id is the ID of the efs filesystem and can be found in the AWS console. /directory-select which directory you would like to mount the filesystem at
 
EFS can be mounted but cannot be booted, can't install an operting system. This is the difference between EFS and EBS volumes
-------------------------------------
Demo: EFS

Create 2 instances in 2 different AZs US-east-1a and US-east-1b . Now lets create a managed EFS and mount it on to both of these servers. They can both be reading and writing to it at the same time. 
Goto EFS --> create file system --> customize --> name: efsdemo --> standard --> Enable automatic backup --> life cycle management -> tarnsition into IA: 30 days since last access --> enable encryption at rest --> throughput mode: Enhanced --> Next --> choose vpc --> select AZs and our efs security group we created (to allow traffic from the sg of our instances) and it will automatically configure mount targets (it is good to have mount targets in all AZ for redundancy) --> create olicy if u want --> create
Now our EFS file system is ready and login to both servers, 
--> sudo mkdir /efsdemo   (in both servers)
--> sudo yum install amazon-efs-utils -y (in both servers)
--> sudo mount.efs fs-08d374b450987 /efsdemo    (the efs id we get from efs console) (in both servers)
--> df -kh 
--> cd /efsdemo
--> sudo vi file1
"server1 text"
--> go to server2 and we can see the file1 there in /efsdemo path
Similarly you can create file2 in this server and check it in server-1

You can also configure fstab for this
----------------------------------------
FSx:

FSx is a fully managed service that provides high performance file storage for wide range of workloads designed to easier for businesses and organizations.
EFS only works with linux. If we have windows based m/c,we should use FSx. 

FSx will provide below features:
* Provisioning file servers and storage volumes
* Replicaing data
* Patching file server
* Addressing hardware issues
* Performing manual backups.

Fsx-Benefits
* storage (store files,docs,photos,videos)
* managed
* Scalable
* shared access
* Backups

Different Flavors of FSx
1. Amazon FSx for Windows file server.
   * It supports the server message Block (SMB) protocol
   * You can easily integrate it with Microsoft Active Directory
   * It supports data deduplication
   * You can set quotas

2. Amazon FSx for lusture.
   * optimized for high performance.
   * It provides low-latency,high-throughput access to data. 
   * It is built on the lusture file system
   * Amazon FSx for lusture integrates seamlessly with other AWS services like Amazon S3,AWS DataSync, and AWS Batch
   * You can easily scale the file system's apacity and throughput

3. Amazon FSx for NetAPP ONTAP.
   * It offers high-performance stprage that's accessible from linux,windows,and macOS via NFS,SMB,and iSCSI protocols
   * It can scale your file system up or down in response to workloads demands
   * It can perform snapshots,lones,Replications and much more

4. Amazon FSx for openZFS
   * It is built on top of the open-source OpenZFS file system
   * It supports access from Linux,windows,and macos via industry standard NFS Protocol
   * It utilizes power openZFS capabilities including data compression,snapshots,and data clonning
   * It offers built-in data protection and security features.

Deployment options for FSx:
* FSx for windows,OnTap,OpenZFS supports both single-AZ and Multi-AZ deployments
* FSx luster supports only single AZ deployment.
-----------------------------------------
S3 overview:
 
Simple Storage Service is used for object storage. It provides Scalability, Data Availability,Security and Performance. (it is similar to dropbox and google drive)
It can integrate easily with other AWS services and also we can use IAM to defile who has access to our files,modify,delete etc. S3 can also we accessed using AWS console,AWS CLI,SDK and rest API. S3 doesnt have file system in it. and also we cannot mount it to servers. 
S3 can store log files,Media Audio/Video/Images,CI/CD Artifacts. 

S3-Use Cases:
When we as a user send request to website, the website will return the HTML file and this file will have link to images and videos which is in web server. We as a user will request the webserver or these images and webserver will fetch it for us. 
If file sizes increases,then it will become very expensive to store these on webservers. And if we had 100s of web servers, then all webservers would need to store images/videos. This becomes problem in our traditional storage. With S3 we can solve this issue.
Consider we have a webserver and it contails HTML files and all the other medias/videos etc are stored on S3. Now we as a user will request the server and get back the html file and it will have a url which will point to s3 and now s3 will respond with all video we want. S3 will now have responsibility of containing,storing,replicating,backing up etc for TB of data. 

S3 bucket is a place or directory which stores files. We can have any number of buckets in our account. multiple bucket for single application and multiple bucket for multiple applications. 
Objects are the files that are uploaded to S3.file is referred to as objects. An object has: key-The file name and Value-File data. Also there are many properties associated with object like VersionID,Metadata,Other information . If we enable versioning in our S3 bucket, we also have version ID associated with it. 
S3 Buckets have a flat file structure. We cannot have nested directories (folder inside folder). It is only one folder and all files are init. But if we see in the console of s3 bucket, we can see file structure like /music/song1,/music/song2 etc but it is just a illusion. It is actually a file structure and the name of the file is /music/song1, /music/song2 etc . S3 is displaying to us in folder like structure on UI. You cannot boot or mount to S3 buckets (IMP)

When we upload a file to S3, In the background AWS will store on some server only on some AZ (we cant see it). And S3 is designed for High availability and durability. It will take care of our files in server. It will have replication of our file in another server and even if it goes down, file copy will be present. It will replicate our files across multiple servers across multiple AZ.

S3 bucket names must be unique globally across all aws account. whenever the new bucket is created, the name of the bucket is embeded in the unique s3 url. If another user tries to create a bucket in same name,it is going to fail because we cannot have same bucket url pointing to two different buckets.

S3 restrictions:
* S3 can handle unlimited number of objects
* maximum size of a single file is 5 TB
* An AWS account supports 100 buckets by default,but this number can be increased to 1000 by requesting a service limit increase. 
----------------------------------------
Demo: S3 basics

go to s3 --> Create bucket (S3 service is global and while creating bucket we specify what region we want that bucket to be in) --> kk-demo-123 --> select region: us-east-1 --> if we have any prior bucket and we can copy existing configurations from it --> by default, for the bucket we create , all the public access are blocked --> create bucket
select the bucket --> properties --> we can see bucket-region,ARN,tags,versioning,encryption,static website hosting,object lock,requester pay etc
Permissions tab wil manage settings of 'who has access to this bucket,all the objects within our bucket'. by default only the creator will have access to it.--> bucket policy 
In metrics tab, we can look at all the cloud watch metrics regarding our bucket like how much data are we using in our bucket (size), how much objects etc .
Management tab we can define life lycle rules,replication rules, inventory configurations.
In Access Points, where it manages access data access at scale for shared datasets in s3. (named network endpoints)
 
Go to objects and upload our first file. You can drag and drop also. We have properties,permission, --> upload
Once the object is uploaded, click on it . we can see region,size,S3 URI,ARN. If we click on object URl we can see our image. but currently it says access denied. This is because S3 is showing the url in the public end point view. If we want to see the image,on top click on Open tab.
Create folder: food --> copy few files of your food --> upload.
Now this is not a actual folder, we can see the s3 uri as s3://kk-demo-123/food/burger.jpg. And we can access this with object url.
To delete file, we can select file and delete. It will be permanently gone.
We can also move our files inside our folder by select the file and actions --> move --> paste the pull path in the destination (or browse it) --> move
To delete the bucket, we need to first empty it and delete it.
-------------------------------------
S3 - Storge classes 
Storage classes provide various level of Data access,resiliency and cost.
All users do not access the data in the same way and also there may be chance of users paying more cost for data that is used/accessed less often.

1. S3 standard storage class.
This is the default storage class. When we upload a file to this storage class,objects are replicated along atleast 3 AZs. which means that it can handle 2 simultaneous AZ failures. 99.999999999% durability . Billed per GB per month of data stored on S3. 
S3 standard files are immediatly accessible. It allows the files to be publicly available also. Uploading data on S3 (ingress) is free and retrieve data from S3 (egress). We are charged per GB. In S3 Standard, there is no retrival fee,no min size,no min duration.

2. S3 standard-IA (Infrequent Access)
Suaitable for Data that are not accessed frequently and they are large in size. Same like standard class, objects are replicated accross 3 AZ and can handle two simultaneous AZ failures. 99.999999999% durability . Billed per GB per month of data stored on S3. Cheaper than S3 standard. 
Files are also immediatly accessible/available when we retrieve them. Files can be made pubilicly available. Uploading data on S3 (ingress) is free and retrieve data from S3 (egress). We are charged per GB. But every time we retrieve the data, we now have the retrieval fee. Minimum duration charge of 30 days. (i.e if we store file in it and deleted after 5 days, we still get charged for 30 days.). Alsothere is minimum size charge of 128 KB per object. (i.e even if our object is 1KB, we are charged the minimum size of 128KB per object) .  

S3 One Zone-IA :
In this type, objects are not replicated across multiple AZs and they will be stored on one AZ only. Cheaper than S3 standard and S3 Standard-IA. Files can be immediatly retrieved within mini seconds. Files can be made publicly available. Only charged for egress nad ingress is free. we have the retrieval fee. Minimum duration charge of 30 days. (i.e if we store file in it and deleted after 5 days, we still get charged for 30 days.). Also there is minimum size charge of 128 KB per object. Designed for IA data. Not required to handle AZ failure. Replication still occurs with AZ.

S3 Glacier-Instant:  (Archive data)
Low cost Option for rarely accessed data. data retrieval is within mili seconds like other storage classes. Performance same as that of S3 standard.objects stored across multiple AZ. Only charged for egress and ingress is free. has a high retrieval fee, minimum duration charge of 90 days. Also there is minimum size charge of 128 KB per object. Suitable for data not required to access frequently but when we access it ,we need it immediatly.

S3 Glacier-Flexible:
This is the first storge class where objects are not immediatly avilable. They cannot be publically accessible. cheaper than S3 standard and so on...per GB egress charge. has a retrieval fee, minimum duration charge of 90 days. Also there is minimum size charge of 40 KB per object. there are 3 options to retrieve our data. Bulk: 5-12 Hours ,Expedit: 1-5 minutes, standard: 3-5 hours. Faster you want ur data,higher will be the retrival fee. During retrieval, objects are stored inS3 standard-IA class temporarily.  

S3 Glacier Deep Archive:
Cheapest storage class where objects are not immediatly avilable. where objects are not immediatly avilable. per GB egress charge. has a retrieval fee, minimum duration charge of 180 days. Also there is minimum size charge of 40 KB per object.  there are 2 options to retrieve our data. Bulk: 48 Hours ,standard: 12 hours. Faster you want ur data,higher will be the retrival fee. During retrieval, objects are stored inS3 standard-IA class temporarily.

S3 Intelligent tiering:
* Automatically reduces storage cost by intelligently moving data to the most cost-effective access tier.
* Apart from the cost of a storage class an object gets assigned to, all objects will also incur a monitoring/automation cost per 1000 objects. 
--------------------------------
Demo: Storage class

Go to S3 --> create bucket 
Upload a file --> under properties , choose one zone -IA --> upload
To change the storage class, we can go to properties --> storage class --> edit --> save
-----------------------------------
S3 Versioning:

Consider we have s3 bucket with 5 files. if we delete file1, it is gone forever. Also if we upload file5 that already exists, its going to be replaced. In these cases,we enable versioning.
Versioning is enabled on buckets and not individual objects. It will be applied to all object inside the bucket. buckets can be in 3 state. Unversioned, Versioning enabled, Versioning Suspended. Once we enable versioning, we cannot disable it. Only we can suspend versioning. And later we can also re enable the suspended version. (IMP)

How versioning works?
When we upload the object to the bucket to which versioning is enabled, S3 will give a version ID to it version ID-1 (long unique string). Incase we upload new object with same name/key ,instead of replacing it, its going to create a new version version ID-2. New object will have version ID-3. The newest version of object is called latest or current version. This is the version the user will see. 
If versioning is disabled,the version ID of files will be set to NULL.
When we delete the object with versioning enabled, Delete Marker will be added. This is the new version of that object which will not delete anything. It just looks like the object was deleted. All previous versions of object will be hided. Now if we want to undo this, we need to delete the delete marker and now we can see the current version of the object.
If we specify the specific version ID when we delete the object, like file1.txt with version ID 2, it will permanently delete that version. 

Versioning Pricing: 
We are charged for all versions of objects stored in our bucket. Consider we have file1.txt with version1 of 10GB abd Version2 of 15 GB, then we will be charged for 25 GB. 

Version Suspending:
When we suspend versioning, all of the version of that object will remain and it wont be deleted automatically. Next if we upload new object (eg. test.txt), then it will be assigned the version ID of NULL. If we now again upload new version of file1.txt , it will have the version ID of NULL. Again uploading the same file will going to replace old file. 

MFA delete:
This is a nob/checkbox for versioning. When this feature is enabled, MFA is required to change the versioning state of the bucket. Its like extra security. MFA is required to delete versions and can only be enabled using CLI.

S3 Versioning protects S3 objects from being accidentally deleted or overwritten by keeping multiple versions of an object in the same bucket. S3 Object Lock can help prevent objects from being deleted or overwritten for a fixed amount of time, or indefinitely.For a company that needs to comply with regulatory requirements that require them to prevent object deletion or modification for a fixed period.(IMP)

----------------------------------------
Demo: Versioning:

Go to S3 and create bucket and keep the versioning disabled now. upload a file (This is version1). Now delete that file we uploaded. Now trhe file is gone.
upload the file again. now make some changes to the file in ur local (This is version2). And now upload the file again. After uploading, try to opn it and you can see the version2 content.  Our fole is over written. Now delete this file1.txt.
Now go to properties and enable versioning. Now again upload file1 with content "this is version1". once we have uploaded, we can see "show version" button. 
Now lets upload the file1 with version2 content. Now if we go to bucket and enable "show version", we can see file1 two times with versionID and time stamps. We can verify by opening them. similarly upload the version3 of that file1. Now we can see 3 versions of same file1.
Now lets delete file (it will not ask permanently delete). Now we cant see any object in our bucket. But we need to enable show version. A new file1 came with the delete marker. 
Now in order to get our file back, we need to delete (permanently delete) the delete marker. Now we can go back and see our file.
We can also select the specific version and delete it (permanently delete).
Now lets see how to suspend the version. go to properties --> edit versioning -->  we see Suspend option --> save.
Now our object will remain as they are. now lets upload the file1 with version4 text. Now we can see file1 at top of otherversion but its version ID is null. now if we upload the version5 of the file1, now the version4 file has been replaced. We can go ahead and delete the verions manually.
Now lets upload new file2 with text "This is version1". File2 has versionID of null. upload the new file2 with version2 text and it will override it.

MFA Delete:
Now go to properties and edit versioning and we can see Multifactor Authentication. We can enable/disable this on AWS CLI only. 
------------------------------------
S3 Bucket Policies (ACL and Resource Policies):
When we create a bucket, it is only accessible for that user and also root user. SO now users from AWS account or different AWS account or public anonymous user wont be able to access it.
Resource Policy Determines who has access to an S3 resource.
S3 Bucket Policy Determines who can have access to the bucket and what Operations they can perform.
Eg: 
To allow Johndoe to specific bucket
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Sid": "AllowGetObjectForJohndoe",
      "Principal": {
        "AWS": "arn:aws:iam::111122223333:user/Johndoe"
      },
      "Action": "s3:GetObject",
      "Resource": "arn:aws:s3:::your-bucket-name/*"
    }
  ]
}
---
To allow all but deny DaisyM
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "AllowAllGetObject",
      "Effect": "Allow",
      "Principal": "*",                    # apply to all users
      "Action": "s3:GetObject",
      "Resource": "arn:aws:s3:::your-bucket-name/*"
    },
    {
      "Sid": "DenyDaisyMGetObject",
      "Effect": "Deny",
      "Principal": {
        "AWS": "arn:aws:iam::111122223333:user/DaisyM"
      },
      "Action": "s3:GetObject",
      "Resource": "arn:aws:s3:::your-bucket-name/*"
    }
  ]
}
---
Daisy user to access files in specific bucket path

{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "AllowDaisyMGetObjectInMediaPrefix",
      "Effect": "Allow",
      "Principal": {
        "AWS": "arn:aws:iam::111122223333:user/DaisyM"
      },
      "Action": "s3:GetObject",
      "Resource": "arn:aws:s3:::EXAMPLE-BUCKET/media/*"
    }
  ]
}
---
Policy to grant access to the bucket for the people IP in specific subnet range

{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "AllowAccessFromSpecificIPRange",
      "Effect": "Allow",
      "Principal": "*",
      "Action": "s3:*",
      "Resource": [
        "arn:aws:s3:::EXAMPLE-BUCKET",
        "arn:aws:s3:::EXAMPLE-BUCKET/*"
      ],
      "Condition": {
        "IpAddress": {
          "aws:SourceIp": "192.0.2.0/24"
        }
      }
    }
  ]
}
---
To give access to specific folders in bucket

{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "AllowAccessToSpecificFolders",
      "Effect": "Allow",
      "Principal": "*",
      "Action": "s3:GetObject",
      "Resource": [
        "arn:aws:s3:::EXAMPLE-BUCKET/audio/*",
        "arn:aws:s3:::EXAMPLE-BUCKET/video/*"
      ],
      "Condition": {
        "StringLike": {
          "s3:prefix": [
            "audio/*",
            "video/*"
          ]
        }
      }
    }
  ]
}
---
Block Public User: Policy to access bucket to only the users present in AWS account.

{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "AllowPublicAccessToBucket",
      "Effect": "Allow",
      "Principal": "*",
      "Action": "s3:GetObject",
      "Resource": "arn:aws:s3:::EXAMPLE-BUCKET/*"
    }
  ]
}

But above policy will make all the public also to access out bucket, for this purpose ,AWS introduced "Block all Public Access" . Untill we uncheck this option in AWS console, whatever the policy we define will not allow public to access our bucket.
Now even if we disable that nob, resource policy is the only thing that will expose the bucket to public. 

IAM Policies VS resource policies:
* IAM policies can only be applied to authenticated AWS users. Cannot be applied to anonymous users.
* Since the resource policy is applied to the resource,rules can be added for anonymous/public users. (eg. s3 bucket,EC2 instance)

IAM policy and Resource policy work together. if a user want to access a bucket, both iam and resource policy should not deny him anywhere.both should allow him.

S3 ACLs:
ACLs are legacy access control mechanism that predates IAM. AWS does not recommend to use it to users. ACLs are inflexible and provide only a limited set of rules. They cannot be applied to group of objects.

If there is any issue for the user to access any bucket, check his/her iam policy.
-----------------------------------
Demo: ACL and Resource policies.

Create 3 users , account1 create user1,account 1 create user2 and in account 2 create user 1.
as user1, create a demo s3 bucket.(disable ACLs and use block public access.) 
Once the bucket is created, upload some files in it. 
Now try to open the file as same user & you can open but object url is denied as expected. Go to IAM policy and check that user2 is allowed to list bucket.
lets see if another user from same account has access to this bucket. user2 can only list out the content of bucket. He cannot open file which user1 uploaded.
Now lets setup policy that allows user2 to have access to only logs folder.
As user1 goto permisions --> edit bucket policy --> you can use policy generator or use the wizard given on right side to get the default template --> 
create a policy with get object for s3 bucket for logs folder ,with sid any name u want,resource as our bucket-arn/log/* , Action as s3getobject ,principal as the user2 ARN. --> create   (you can also make use of right wizard for policy)
Now as user2, go to s3 and check your access. we can only access files in logs folder and restricted for other folders. Now lets give access to user to delete any file in traces folder. Actions: deleteobject ,Resource: bucket-arn/trace/* --> save
Now we can delete file in trace folder.
Note: If we are using 2 actions as a list like s3getobject and s3deletebucket, we need to pass 2 resources aswell like  ["bucket-arn/log/*", "bucket-arn"].
Next inorder to make access of our object url to public, as a user1, go to bucket policy and add more policy with principal: *,actions: s3getobjects, sid: allowpublic,resource: bucketarn/media/* -->save
But inorder to disable this, first we need to uncheck the block all public access.Now this allows us to define a policy that allows public access to bucket.
Now go to media folder and access the objecturl.
Next lets create policy to allow user from different account to access this bucket.
Loginto another aws account as user1 and open cloud shell 
--> aws s3 ls 
--> aws s3 ls s3://kk-resource-policies       (# o/p: error)
Now come to aws account1 and add new statement for our bucket policy with principal: <paste-arn-of-user1-from-account-2>,Effect : Allow, Actions: ["s3listbucket","s3DeleteObject"] --> Resource: ["bucket-arn/log/*", "bucket-arn"] . 
Now in the cloudshell, --> aws s3 ls s3://kk-resource-policies   (#o/p: we can see contents)
--> aws s3 rm s3://kk-resource-policies/file1.txt  (# access denied)
--> aws s3 rm s3://kk-resource-policies/logs/log1   (# log1 deleted)
-------------------------------------
S3-Static Website hosting

The way the website works is we as a user will send HTTP GET request to webserver and webserver will respond with an HTML file. 
Note: Website is nothing but html file that the webbrowser renders onto screen. 
The components that makeup the website is 
1. html file : Structure/content of the website
2. css file : Adds colors and provides visual elements
3. Javascript : Adds dynamic functionality
4. Images/videos/audio

All the above things can be stored in S3 bucket. S3 has a Knob that allows to host html,css,js files on a bucket. This allows access to website files through HTTP. S3 will give the URL through which you can access the website.
NOTE: It is only used for static Websites. If our webside needs any server side logic, then we need to deploy it in EC2,EKS,ECS,lambda etc
To customize a domain for your website,the bucket must follow a specific format

Pricing:
Price/GB (storage) & price/GB (egress) . Also we should pay per request sent to this static site. 0.0004$ for every 1000 get request in S3 standard SC.

Custom Domain name:
once we host our static website, we get a url in format http://bucketname.s3-website-<region-name>.amazonaws.com . To use a custom domain name, use route53 . If the domain name we have is http://bestcars.com ,then our bucket name should also be bestcars.com .
--------------------------------------
Demo: S3-Static website hosting.

gather all required files like index.html,index.css,404.html and all images we want and keep all these at one place.
go to S3 and create a bucket "kk-static-demo". Upload all the website files to this. Now go to properties --> static website hosting --> edit --> enable --> host a static website --> Index document : index.html --> Error document: 404.html --> save
Now we get a url for static website hosting . If we click it, we see access denied. 
Lets setup resource policy. first uncheck block public access --> come to bucket policy and create -->principal : *, Effect: Allow , Action: s3getobject , resource: arn-bucket/* --> save  (Now we can see publicly accessible)

Now try to access the static url and we can see our website. we no need to specify url/index.html here this is because we setup index file. Now enter url/jnscns and we can see 404 page not found
-------------------------------------
S3 - Pre signed Urls:

Consider we have a private bucket in Aws account. me as a authenticated user, i can access this bucket,upload objects etc. Suppose there are couple of files in this account and we need to share it with someone who doesn't have any aws account and he is a public user. To share those files with the user, 
(1) We can create this user and give access key,secret key (but this is not correct in security point of view). And if there re multiple people,this become hectic. 
(2) To make bucket public. (all public users will have access but we want only couple of users to access this)

This is where pre signed urls come in handy. We as an autherticated IAM user,we can create a pre signed url for our bucket. We send s3 the request and s3 will give us the pre signed url. And this url we can share it to public who needs access to an object. Now when the user sends back the request to object in bucket, S3 will think that as we(the authenticated user). Now other public cant access this bucket as they dont have url. 

Pre signed url use case:
(1) Consider we have server that contains 1000GB of video ideally in s3 bucket. We as a customer can send request to webserver  and webserver can generate pre signed url as a specific user who has access to the video "userx". Server can send that pre signed url as userx and now we can send the request to s3 bucket which will look like we are user x and now we can watch those videos. 
Presigned urls can also be used for uploading objects to s3 bucket as well.
(2) Consider we have some account on website and we want to update our profile pic. We have a webserver with api and we have S3 bucket which stores all the profile pics of users. When we want to upload a profile pic, we will first send our profile pic to api and api server will send it to s3 bucket.
Note: This requires all files to traverse through back-end servers. We now may need to cutoff this middle men. Thsi is where we can use s3 presigned url . Where user can send request to API server and API server can generate pre signed url that will allow user to upload a file to bucket. Then user is able to upload a file directly to bucket. 

NOTE: 
1. When creating pre-signed URLs, an expiration date must be provided.
2. Expiration duration of maximum 7 days using an IAM user is provided
3. If an IAM user does not have access to S3 bucket, a pre-signed URL can still be generated using that ccount.
4. The pre-signed IRL does not give you access to a bucket;however,it allows you to send arequest to S3 as that user that generated the URL.

Consider we have a IAM user who doesn't have access to S3 bucket, If this user creates a presigned url and sends it to another user and that user tries to send that request to the s3 bucket, he will be denied.
------------------------------------
Demo: S3-Pre-signed-Url:

Create a bucket kk-pre-signed-demo --> upload a image --> upload
Click on the image --> object actions --> share with a pre signed url --> enter time period: 30 min --> create presigned url --> copy the pre signed url --> paste it in browser and we can see the image. Now anybody who has access to this url can access this for 30 min. Most of the time we use AWS SDK or CLI to create presigned url. 
Now consider we have user2 and he has bucket policy to only list the bucket and object inside it. but now this user can also generate pre signed url for himself but it wont work. 
after login in as that user2, go to object properties --> object actions --> generate pre signed url --> period: 30 min --> create
copy it and paste it in browser and it says access denied.
----------------------------------
S3 -Access points:

Consider we have a bucket and all developers,admin,infra,legal groups etc want to access it as they are developing something.They all have seperate policies for them.And now if we create bucket policy it will be very complex.
For addressing this, we can create a access point and each one of these group get their own access points. Through access points, they get their own little view/tunnel into the bucket. Svery access point gives its own unique ARN. And now developers who want access to this bucket will point to the ARN of the Access point which operates like a kind of S3 bucket. Great point of having acces point is we can define access policies on each of the access points.

Access point Restricting VPCs:
One more benifit is suppose we have a bucket and we want to restricts its access to only users from specific vpc, then we can do this by using access points and VPC end points where we can define certain rules to allow only traffic from certain vpc to our bucket.

When we define access policy, we need to define it on both access point and bucket policy. Same policy needs to be copied to bucket policy. Instead we can define the policy on bucket to deligate policies to access point.
-----------------------------------
Demo: Access Points

Create a bucket kk-access-points . Upload one demo image.
Create 3 users user1,user2,user3 all in same account. Login to cloud shell of user2 and user3 and check if they have access to our bucket. And they dont.
Go to user1 and create a access point for our bucket. in the left hamburger menu. name: developer --> choose bucket in this account: kk-access-points --> Network origin: VPC or internet (choose) --> skip the policy for now and create
similarly create one more access point and name it finance
Go to s3 documentation and under configuring IAM policy, we can get access point policy examples. The access point policy looks almost similar to bucket policy but in the Resource section, we can see it doesn't points to S3 bucket instead it points to access point policy.
Now instead of copying same policy to bucket policy everytime, we can instead delegate access control to the bucket to the access point.
* bucket policy that delegate access control to the access point
{
    "Version": "2012-10-17",
    "Statement" : [
    {
        "Effect": "Allow",
        "Principal" : { "AWS": "*" },
        "Action" : "*",
        "Resource" : [ "Bucket ARN", "Bucket ARN/*"],
        "Condition": {
            "StringEquals" : { "s3:DataAccessPointAccount" : "Bucket owner's account ID" }
        }
    }]
} 
---
copy paste above policy to bucket policy of ur bucket. and save it.
Next come to access point of developers and add the below policy to it to allow user2
{
    "Version":"2012-10-17",
    "Statement": [
    {
        "Effect": "Allow",
        "Principal": {
            "AWS": "arn:aws:iam::123456789012:user/user2"
        },
        "Action": ["s3:GetObject", "s3:PutObject", "s3:ListBucket"],
        "Resource": ["arn:aws:s3:us-west-2:123456789012:accesspoint/developers/object/*"  
                            "arn:aws:s3:us-west-2:123456789012:accesspoint/developers" ]   # mandatory to add "object" for ARN
    }]
}
---
Now user2 should be able to perform these operations using Access point
Now grab the ARN of the access point, login to cloudshell
--> aws s3 ls s3://<arn-of-access-point>    (#o/p: beach.jpg) 
to copy it to our local
--> aws s3 cp s3://<arn-of-access-point> .
--> ls                (#o/p: beach.jpg)
Now this access point url is reserved for our developers. copy the same policy and paste it in Finance access point and chane all necessary things accordingly.
{
    "Version":"2012-10-17",
    "Statement": [
    {
        "Effect": "Allow",
        "Principal": {
            "AWS": "arn:aws:iam::123456789012:user/user3"
        },
        "Action": ["s3:GetObject", "s3:PutObject", "s3:ListBucket"],
        "Resource": ["arn:aws:s3:us-west-2:123456789012:accesspoint/finance/object/*"  
                            "arn:aws:s3:us-west-2:123456789012:accesspoint/finance" ]   # mandatory to add "object" for ARN
    }]
}
---
Now copy the arn of "finance access point". and in cloud shell
--> aws s3 ls s3://<arn-of-access-point>
--> aws s3 cp s3://<arn-of-access-point> .
--> ls         (# beach.jpg)
--> touch test1
--> aws s3 cp test1 s3://<arn-of-access-point-of-finance>   # copying from local to finance access point
--> aws s3 ls s3://<arn-of-access-point>   (# o/p: beach.jpg test1)
Like this we can interect using accesspoint of bucket
--------------------------------------------
AWS Backup:

Disaster recovery is the process of planning for and responding to events that could cause data loss or system downtime. Disasters can be natural or man made.
Importance of disaster recovery:
Downtime | Data loss, finalcial loss,Damage to reputation and legal issues.
Solid Disaster recovery Plan:
Business Continuity,Minimize Downtime and safeguard data integrety.

Backup vs disaster recovery:
* Backup creates copies of data to restore it in case of data loss. It is an essential part of disaster recovery.
* Disaster recovery Encompasses a broader stratergy,including backup. It includes planning for system and application recovery

AWS disaster recovery services are designed to be flexible, scalable and cost effective.
* S3 for disaster recovery:
S3 provides scalable,durable,highly available solutions. S3 offers 99.999999999% of data durability, it is abl to withstand the loss of multiple data centers, replicate data across Multiple AWS availability zones.
* EBS Snapshots for disaster recovery:
Point-in-time copies,EC2 instance and data protection.
Manually or automated on a schedule, saving storage costs,New EBS volumes for recovery.
AWS Backup is a service that makes it easy to centralize or automate the backup of data across aws services and resources.
We get, 
* Single,unified console for managing AWS services,
* Automates backup scheduling and retention policies
* Different regions and different accounts

Basic components and features:
there are 3 components 
-> Backup vault: its a folder which stores our backup data. We can have multiple vaults across regions
-> Backup plan : Defines the configuration for our backup. like schedules,retention policies, backup vault to be used etc
-> Recovery Point: Point in time in which data can be restored.

Consider in us-east-1a, our app is deployed, in which there are 2 ec2 instances,EBS,EFS,RDS. To use AWS backup, we first need to create a backup plan and as well as backup vault which is in same AZ. In backup plan we define that we want to store all our resources in app1 and send it to vault. We can also define scheduling for our backups. Now we can also create a backup vault in another region/another AWS account and we can copy the backups here. And now we can perform recovery of data loss of app1 to either region. 

Basic integretions with other services:
AWS backup support various different resource types.
AWS backup - monitoring integrations like AWS organizations, AWS event bridge,AWS  cloud trail and AWS SNS.
--------------------------------------------
Elastic Disaster recovery:

It is a fully managed service that provides fast,reliable recovery of on premesis and cloud applications using affordable storage,Minimal compute and point-in-time recovery. Also It offers to use AWS as a recovery site, Keep things in a continual replication state,easy to access with a disaster recovery infrastructure.
We have to tell AWS to failover to all the data that we have backed up and it will all be done within no time and now all the traffic goes to disaster recovery site. We can use Elastic disaster recovery to recover data from On-Premise to AWS, From other cloud patforms (GCP,Azure) to AWS, From one AWS region to another.

How DRS works?
First we need to define what services to be monitored for failover and they are called as source servers. And for all of these source servers, we need to be able to replicate data in AWS cloud and for this we need to intall AWS replication Agent. Next we need to define where is our staging Area. Staging Area is a subnet where ec2 instances are deployed so that they can receive the replicated data from  source servers. 
There will be 3 different components, on prem env,staging area and subnet to failover. The recovery servers are the services that we failover to during disaster.
Now finally we have to define the launch settings which is the ec2 settings for your recovery servers.here we specify specifications/locations,Instance/size,region/subnet and Security Groups
----------------------------
Storage Gateway:
It is the hybrid cloud storage service. It acts as a bridge between our on prem env and our cloud based storage allowing us to seamlessly integrate on prem applications with cloud storage resources.
Storage gateway can be used for following purposes:
* An extension for your on-premises Storage needs
* Assists migration into the coud
* Backups
* Disaster recovery

Storage gateway can be either a virtual machine or a physical unit that we can deploy on our on prem datacenter. SG comes in 3 types volume,file and tape. It depends on what kind of storage technology (EBS,nfs) we are using in our on prem env. 

Storage gateway- Volume:
two types -> cached mode and stored mode

in volume stored mode,consider we have on prem datacenter and we have bunch of servers in it. And these servers connect to network attached storage using iscsi protocol. We can created a file system on top of this network attached storage. 
Businesses will ideally like to have some backups to store data as well as for disaster recovery. Implementing disaster recovery in on prem becomes changelling. For this purpose we can make use of storage gateway. Once SG is deployed , we can use SG to assist us to take backup and disaster recovery. In Stored mode, the gateway appliance presents storage volumes for physical servers using iSCSI protocol. Now the physical storage will think that the storage gateway is also some type of network attached storage which they can create file system on top.
Note that in volume stored mode, the volumes that are presented to servers are all consume capacity on prem. Meaning that the storage gateway has physical drives attached to it. Once the data is written to the disk, the storge gateway will copy the data to AWS via storage gateway end point and then the data is copied to S3 as EBS snapshots. with those snapshots, we can create new volume and attach it to EC2 instance and get complete replica of on prem environment.

Points to be noted:
* Data is stored locally on-prem. Stored on physical disks that is attached to the storage gateway on prem.(IMP)
* Data is replicated asynchronously to AWS S3.
* Provides convenient backup of data
* Assists with disaster recovery. (create EBS volumes from snapshots)
* Doesn't increase datacenter storage capacity. (All data is still stored on-prem,only backups are stored in AWS)

Storage Gateway - Volume cached mode:
We will have same setup as that of stored mode,where we have deployed storage gateway on prem environment and we are presenting block storage to the main servers connected through iSCSI. But only difference in casched mode is unlike stored mode, we dont actually store data locally. All data are stored in Aws in S3. It will look like it is stored on prem in perspective of servers.We will have local cached storage in on premise environment which handles files that are accessed frequently. Now since all data is stored in cloud, it helps us to extend the overall size of storage solution and infinetly scale up. 

Points to be noted:
* Data is stored on S3
* Only data on-prem is cached data, for frequently accessed data.
* Cached mode acts as a datacenter extension increases customers storage capacity.

Storage Gateway - File:
In this type, in on prem, Servers will connect to file system by using a protocol  like NFS/SMB. It is going to deploy a virtual machine/physical server into the on prem env. And final storage gateway will present itself as file system storage that servers can connect to using SMB. Servers can now write to storage gateway and any of the created files are not stored locally and instead it will go and store to AWS s3 (eg. /media/pic1.jpg). This acts like extention to our data center so that we can infinetly scale up.

Points to be noted:
* Stored in S3
* Only data on-prem is cached, for frequently accessed data. 

Storage Gateway - Tape:

In this type, the servers connect to tape using iSCSI protocol. This will perform secure back up of data in a tape. We deploy our storage gateway on prem and it will present like a tape backup and the data will be emulated as tape drive.
Whenever we are go to create backup, it will store it in AWS S3. It will store in VTL (Virtual Tape Library) which is running in S3 bucket. For less frequently access data we can move it to VTS (Virtual tape shelf).  

Points to be noted:
* Emulates a tape library
* Data is stored in AWS (VTS and VTL)
* Virtual Tape 100GB - 5TB
----------------------------------------
** QUIZ **

* An EC2 instance requires temporary storage of data that changes frequently and is only needed during the instance lifecycle.Amazon instance store is used for this.
* Amazon FSx for Lustre is optimized for high-performance computing (HPC), machine learning, and media data processing workflows, providing a high-performance file system used in compute-intensive environments.Also a fully managed native Microsoft Windows file system to provide shared file storage with the compatibility and features of Windows Server.
* A company wants to analyze their S3 object access patterns to determine when to transition objects to less expensive storage classes.Storage class analysis can be used.
* A data engineer needs to upload large files over a high-latency network to S3. Which method should they use to maximize the upload efficiency? S3 multipart upload
* Which EC2 instance type provides direct-attached Instance Store volumes that are ideal for high random I/O performance and low-latency workloads? Storage-optimized instances
* A database administrator is deploying a high-performance OLTP database system on AWS. Which Amazon EBS volume type should they choose to ensure consistent I/O performance and low latency? provisioned IOPS SSD (io2)
* An organization wants to enforce that all objects uploaded to their S3 bucket are encrypted at rest. Which method can they use to achieve this? use s3 bucket policy to deny uploads that are not encrypted.
* S3 Lifecycle Policies can automate the transition of objects to a cheaper storage class after a certain period and define rules for automatic deletion after a specified time.
* AWS Elastic Disaster Recovery minimizes downtime and data loss with fast, reliable recovery of physical, virtual, and cloud-based servers into AWS
* S3 Access Control Lists (ACLs) can grant read and write permissions to a single user on specific S3 objects, allowing for fine-grained access control. Use Bucket Policies for Complex Scenarios where as Use ACLs for Simple Access Control.
* A developer wants to trigger a Lambda function for processing images each time a new image is uploaded to their S3 bucket. Which feature should they configure to achieve this? S3 event notification
* A web development company wants to improve the upload speed to their S3 bucket for global users. Which feature should they enable on their S3 bucket? S3 transfer acceleration
* Amazon FSx for NetApp ONTAP offers full compatibility with NetApp management tools and features, allowing for easy migration of on-premises NetApp file storage to AWS.
* An organization is looking for a way to save on storage costs for objects with unknown or changing access patterns. Which S3 storage class automatically moves objects between two access tiers based on changing access patterns? S3 intelligent tiering
* A company wants to connect an on-premises software application with cloud-based storage to provide secure and scalable storage infrastructure. Which hybrid cloud storage service should they implement? Amazon storage gateway
* A multinational company requires their S3 data to be replicated across regions for lower-latency access and compliance with data sovereignty requirements. Which S3 feature should they implement? S3 cross region replication (CRR)
-----------------------------------------------------------------------------------------------------------------------------------------------------------
**** Services: Compute

EC2:
When we create a application, we need it to be deployed on a server. It should be able to handle all the client request and handle responses. 
The term server comes from a client server architecture. where client send request to server and and get the response back. In webserver, the web browser will go to ip address or the domain name of specific website which will get routed to the server and server will serve the html file that represents the website by sending that back as a response. Once we deploy our app into a server, the end users.clients will send request to our server and they will get the response of data that they are ultimately looking for. The end users can be web browser or mobile application and they will send request to our server.
Aws allows us to rent the server in the cloud i.e EC2 instance.

Different EC2 instance types are 
* General purpose (webservers and other traditional apps)
* Compute optimized (when we need heigher CPU)
* Memory optimized (fast performances of workloads) (heigher RAM)
* Storage optimized (for High IO operations applications for more read and write performances)
* GPU instances (for machine learning,Deep learning which needs extra GPU)

Amazon Machine Image (AMI):
When we deploy a server,we need to install a OS. We utilize AMIs which tell what type of OS is installed on server like ubuntu,amazon linux etc. We can also customize AMIs like we can tell that we need webserver on top of OS when we launch the instance.Also we can get softwares and dependency files with AMIs. Then with one AMI, we can create as many instance as we want. All will be identical.

Types of AMI:
* Public AMI (publically accessible),
* private AMI (only owners of the AMI can access it)
* Shared AMIs (can be shared across different organizations or accounts)

If we have a ec2 instance and we have made our own changes, we can create our own  AMIs. 
One instance is created , we can login to it using SSH. we need to also specify keypair. Public Key and private key. Public key is already created in instance. We can use private key to login to EC2 instance securely. This will encrypt the login information.

EC2 instance life cycle:
When we first launch EC2 instance, it will be in pending --> Running --> Stopped --> starting --> shutting Down --> terminated

* User data:
At the time of launching EC2 instance, we can pass user data into it. it contains shell script,download any files necessary to download, any pre configurations,install software.
We should also create security group for our instance.

* EC2 with EBS:
Scalable block storage ,persistent storage to store data. also we get snapshots. They are incremental.

* Ec2 instances can also integrate with two other services like ELB and ASG. If we deploy our applications across multiple instance and even across multiple AZ, the load balancer will balance load across them. Autoscaling group allows us to automatically adjust the number of instances within the group.

* EC2 with ElasticIPs. We can associate one elastic IP with one instance and then dettach it and again attach it to other instance. 

* EC2 launch template - If we want our EC2 to be deployed with specific AMIs and specific configuration, we can use this. Auto scaling group needs to know what configuration need to be created for the newly launched instance, then it can make use of launch template.

* EC2 instance placements:
When we create a EC2 instance, it can be deployed in any server in a data center or even across any AZ. Here we can give AWS some configuration to tell that we want our EC2 to be placed in specific location depending on what type of application you have. 
1. cluster placement group : will place our EC2 instances as close to each other as possible (In same rack AZ). Suitable for app which needs low network latency, high network throughput where we need high performance computing applications like big data snd analytics for workloads.
2. Partition placement group: This spreads instances across logical partitions which ensures that instances in one partition do not share the underlying hardware with instances in other partitions. Suitable for distributed replicated or hadoop based workloads. Each rack will have is own network and power source. Partitions wont be in same hardware.
3. Spread placement group: This spreads out our instances across distinct underlying hardware to reduce core related failures. This is ideal for small number of critical instances that should be kept seperate from each other. In this it ensures that each instance is placed on distinct rack and each rack will have its own network and power source.

EC2 instance purchasing option:
On demand, Spot,Saving plans,reserved instances,dedicated hosts,dedicated instances.
* On demand: allows us to pay for the compute capacity by the hour or the second with no long time commitments or no upfront payments. Suitable for short term or irregular work loads that cannot be interupted. Benifits are you get flexibility  like increased/decreased capacity 
* Spot instances: It allows you to bid on unused ec2 instances on discount.They are available upto 90% discount compared to on demand instances. We will be not sure when we will get spot instances. It can go away whenever and it is unpredictable. It can be used for batch processing that can run whenever it needs to run.
* savings plan:  Commit to spend a specific amount of EC2 compute capacity for a 1-year or 3-year term in exchange for a lower hourly rate. Savings Plans provide flexibility by allowing you to switch instance types and regions.
Cost: Generally more cost-effective than On-Demand, especially if you commit to a consistent level of usage.
* Reserved Instance: (commiting to use)
Reserve a specific instance type in a specific region for a 1-year or 3-year term. Reserved Instances provide significant discounts over On-Demand pricing. There are three payment options: All Upfront, Partial Upfront, and No Upfront.
Cost: Less expensive than On-Demand, with cost savings increasing based on the payment option and term length.
* Dedicated host: 
Purchase physical servers dedicated to your use. Dedicated Hosts allow you to use your own software licenses and meet specific compliance requirements. We always get the same exact host with the same exact serial number even if we stop and start the instances.
Cost: Typically more expensive than other options due to the dedicated nature of the hardware and associated management costs. 
* Dedicated Instance:
Instances run on hardware that is dedicated to your use, but you dont have control over the underlying host. Dedicated Instances do not share hardware with other AWS accounts. The hosts on which our ec2 instances are can change.If we shut our EC2 instances,and again start them, they can move to another physical host. The host (physical server) is reserved exclusivly for us. but which specific host can change over time.
Cost: More expensive than standard On-Demand instances due to the dedicated nature of the hardware.
---------------------------------
Demo: EC2

Go to EC2 --> instances --> launch --> give name,choose ami,choose keypair,choose vpc subnet,choose volume,create security group --> create instance "ec2-demo-instance"
Note that ami id will change based on region.
ssh into the instance
--> ssh -i <pem-file.pem> ec2-user@<public-ip of instance>
Terminate the instance
---------------------------------
EC2 image Builder


